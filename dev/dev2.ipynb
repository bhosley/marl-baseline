{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm VF Functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-04 19:34:25,483\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-11-04 19:34:25,944\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/opt/conda/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/opt/conda/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2024-11-04 19:34:29,556\tWARNING algorithm_config.py:4415 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "/opt/conda/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:568: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-11-04 19:34:30,937\tWARNING services.py:2022 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67084288 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-11-04 19:34:31,078\tINFO worker.py:1807 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(RolloutWorker pid=1607720)\u001b[0m 2024-11-04 19:34:34,878\tWARNING deprecation.py:50 -- DeprecationWarning: `get_agent_ids` has been deprecated. Use `MultiAgentEnv.possible_agents` instead. This will raise an error in the future!\n",
      "2024-11-04 19:34:35,793\tWARNING algorithm_config.py:4415 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "2024-11-04 19:34:35,851\tWARNING deprecation.py:50 -- DeprecationWarning: `get_agent_ids` has been deprecated. Use `MultiAgentEnv.possible_agents` instead. This will raise an error in the future!\n",
      "2024-11-04 19:34:35,963\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0 eval = 157.5417865937023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 19:35:35,617\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1 train = 114.53457892178979\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from glob import glob\n",
    "from os import path\n",
    "\n",
    "import ray\n",
    "from ray.tune import Trainable, Tuner\n",
    "from ray.tune.registry import register_trainable, validate_trainable\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "\n",
    "from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "\n",
    "parser = add_rllib_example_script_args(\n",
    "    default_iters=10,\n",
    "    default_timesteps=1000000,\n",
    "    default_reward=300,\n",
    ")\n",
    "args = parser.parse_args(args=[])\n",
    "args.num_env_runners = 10\n",
    "args.env = 'waterworld'\n",
    "args.algo = 'PPO'\n",
    "args.num_agents = 4\n",
    "args.test_agents = 4\n",
    "\n",
    "checkpoint_path = f\"/root/test/{args.env}/{args.algo}/{args.num_agents}_agent\"\n",
    "\n",
    "sup = sorted(glob(checkpoint_path+'/*'))[0]\n",
    "\n",
    "pols = glob(sup+\"/policies/*\")\n",
    "specs = {path.basename(p) : Policy.from_checkpoint(p) for p in pols}\n",
    "\n",
    "register_env(f\"{args.num_agents}_agent_env\", lambda _: ParallelPettingZooEnv(waterworld_v4.parallel_env(n_pursuers=args.num_agents)))\n",
    "policies = {f\"pursuer_{i}\" for i in range(args.num_agents)}\n",
    "\n",
    "\n",
    "resto_config = (\n",
    "    get_trainable_cls(\"PPO\")\n",
    "    .get_default_config()\n",
    "    .environment(f\"{args.num_agents}_agent_env\")\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=(lambda aid, *args, **kwargs: aid),\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=MultiRLModuleSpec(\n",
    "            rl_module_specs={p: RLModuleSpec() for p in policies},\n",
    "        ),\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "    )\n",
    ")\n",
    "resto_algo = resto_config.build()\n",
    "\"\"\" Known-good weight transfer\n",
    "\n",
    "for test_id in range(args.test_agents):\n",
    "    train_id = np.random.randint(args.num_agents)\n",
    "    resto_algo.get_policy(f\"pursuer_{test_id}\").set_weights(specs[f\"pursuer_{train_id}\"].get_weights())\n",
    "\"\"\"\n",
    "\n",
    "for test_id in range(args.test_agents):\n",
    "    resto_algo.remove_policy(f\"pursuer_{test_id}\")\n",
    "    resto_algo.add_policy(f\"pursuer_{test_id}\", policy=specs[f\"pursuer_{test_id}\"])\n",
    "\n",
    "print(f\"Iter 0 eval = {resto_algo.evaluate()['env_runners']['episode_reward_mean']}\")\n",
    "print(f\"Iter 1 train = {resto_algo.train()['env_runners']['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env_runners': {'episode_reward_max': 461.5181588385271,\n",
       "  'episode_reward_min': -20.753103857376605,\n",
       "  'episode_reward_mean': 217.74148432444377,\n",
       "  'episode_len_mean': 500.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_timesteps_total': 5000,\n",
       "  'policy_reward_min': {'pursuer_0': -6.865795740206084,\n",
       "   'pursuer_1': -35.87920937729004,\n",
       "   'pursuer_2': -3.864316381727646,\n",
       "   'pursuer_3': -120.14498397644783},\n",
       "  'policy_reward_max': {'pursuer_0': 230.30065341508384,\n",
       "   'pursuer_1': 177.46971310653652,\n",
       "   'pursuer_2': 205.22379008417556,\n",
       "   'pursuer_3': 75.05780568295731},\n",
       "  'policy_reward_mean': {'pursuer_0': 107.08053636479556,\n",
       "   'pursuer_1': 78.92683764095337,\n",
       "   'pursuer_2': 71.6896900930275,\n",
       "   'pursuer_3': -39.955579774332435},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [217.63442235367253,\n",
       "    253.2640716211442,\n",
       "    461.5181588385271,\n",
       "    175.2852620876895,\n",
       "    -20.753103857376605,\n",
       "    13.641541068033922,\n",
       "    172.43116994939476,\n",
       "    285.90102996606805,\n",
       "    296.82876962081303,\n",
       "    321.6635215964709],\n",
       "   'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500],\n",
       "   'policy_pursuer_0_reward': [84.83073615268276,\n",
       "    212.99594712087287,\n",
       "    230.30065341508384,\n",
       "    48.71389847088052,\n",
       "    -6.865795740206084,\n",
       "    33.64123506220572,\n",
       "    114.21696085677803,\n",
       "    153.79908734515053,\n",
       "    86.13673119200767,\n",
       "    113.03590977249969],\n",
       "   'policy_pursuer_1_reward': [54.00003094006896,\n",
       "    75.27046433015201,\n",
       "    120.83297628759945,\n",
       "    70.82043180285766,\n",
       "    110.12199224100509,\n",
       "    -35.87920937729004,\n",
       "    76.33837795671928,\n",
       "    177.46971310653652,\n",
       "    87.40953940129475,\n",
       "    52.884059720589924],\n",
       "   'policy_pursuer_2_reward': [141.99265895203348,\n",
       "    24.42354164309123,\n",
       "    205.22379008417556,\n",
       "    61.78346554678783,\n",
       "    -3.864316381727646,\n",
       "    70.23991361273256,\n",
       "    11.035629300693431,\n",
       "    43.81306685196845,\n",
       "    81.56340490009573,\n",
       "    80.68574642042427],\n",
       "   'policy_pursuer_3_reward': [-63.189003691111786,\n",
       "    -59.4258814729719,\n",
       "    -94.83926094833114,\n",
       "    -6.032533732836856,\n",
       "    -120.14498397644783,\n",
       "    -54.360398229614354,\n",
       "    -29.159798164795966,\n",
       "    -89.18083733758762,\n",
       "    41.71909412741585,\n",
       "    75.05780568295731]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.6628343997490785,\n",
       "   'mean_inference_ms': 2.622429739592243,\n",
       "   'mean_action_processing_ms': 0.39185402131609814,\n",
       "   'mean_env_wait_ms': 4.765984917182823,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': 0.004787445068359375,\n",
       "   'StateBufferConnector_ms': 0.0022292137145996094,\n",
       "   'ViewRequirementAgentConnector_ms': 0.05525529384613037},\n",
       "  'num_episodes': 10,\n",
       "  'episode_return_max': 461.5181588385271,\n",
       "  'episode_return_min': -20.753103857376605,\n",
       "  'episode_return_mean': 217.74148432444377,\n",
       "  'episodes_this_iter': 10},\n",
       " 'num_agent_steps_sampled_this_iter': 20000,\n",
       " 'num_env_steps_sampled_this_iter': 5000,\n",
       " 'timesteps_this_iter': 5000}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Iter 0 eval = {resto_algo.evaluate()['env_runners']['episode_reward_max']}\")\n",
    "print(f\"Iter 1 train = {resto_algo.train()['env_runners']['episode_reward_max']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resto_algo.reward_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_logits._model.0.weight', '_logits._model.0.bias', '_hidden_layers.0._model.0.weight', '_hidden_layers.0._model.0.bias', '_hidden_layers.1._model.0.weight', '_hidden_layers.1._model.0.bias', '_value_branch_separate.0._model.0.weight', '_value_branch_separate.0._model.0.bias', '_value_branch_separate.1._model.0.weight', '_value_branch_separate.1._model.0.bias', '_value_branch._model.0.weight', '_value_branch._model.0.bias'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resto_algo.get_policy('pursuer_0').get_weights().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:eyo2zp24) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sweet-yogurt-2</strong> at: <a href='https://wandb.ai/no-organization-for-signup/delete_me/runs/eyo2zp24' target=\"_blank\">https://wandb.ai/no-organization-for-signup/delete_me/runs/eyo2zp24</a><br/> View project at: <a href='https://wandb.ai/no-organization-for-signup/delete_me' target=\"_blank\">https://wandb.ai/no-organization-for-signup/delete_me</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241105_025845-eyo2zp24/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:eyo2zp24). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/marl-baseline/dev/wandb/run-20241105_025934-e18niwb3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/no-organization-for-signup/delete_me/runs/e18niwb3' target=\"_blank\">jumping-yogurt-3</a></strong> to <a href='https://wandb.ai/no-organization-for-signup/delete_me' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/no-organization-for-signup/delete_me' target=\"_blank\">https://wandb.ai/no-organization-for-signup/delete_me</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/no-organization-for-signup/delete_me/runs/e18niwb3' target=\"_blank\">https://wandb.ai/no-organization-for-signup/delete_me/runs/e18niwb3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 02:59:38,187\tWARNING algorithm_config.py:4415 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "/opt/conda/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:568: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please pass the project name as argument or through the WANDB_PROJECT_NAME environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 101\u001b[0m\n\u001b[1;32m     97\u001b[0m conf\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m     99\u001b[0m setup_wandb(conf, args\u001b[38;5;241m.\u001b[39mwandb_key, project\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mwandb_project)\n\u001b[0;32m--> 101\u001b[0m resto_algo \u001b[38;5;241m=\u001b[39m \u001b[43mresto_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm_config.py:952\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    950\u001b[0m     algo_class \u001b[38;5;241m=\u001b[39m get_trainable_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class)\n\u001b[0;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:584\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;66;03m# Evaluation EnvRunnerGroup and metrics last returned by `self.evaluate()`.\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_env_runner_group: Optional[EnvRunnerGroup] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/tune/trainable/trainable.py:158\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    154\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_logfiles(stdout_file, stderr_file)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:629\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_usage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;66;03m# Create the callbacks object.\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlog_level \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    632\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent log_level is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlog_level\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For more information, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_level\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINFO\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m / \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or use the -v and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-vv flags.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    636\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[34], line 77\u001b[0m, in \u001b[0;36mWandbCallbackWrapper.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m DefaultCallbacks\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     76\u001b[0m WandbLoggerCallback\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/air/integrations/wandb.py:591\u001b[0m, in \u001b[0;36mWandbLoggerCallback.setup\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject \u001b[38;5;241m=\u001b[39m _get_wandb_project(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject:\n\u001b[0;32m--> 591\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    592\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass the project name as argument or through \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWANDB_PROJECT_ENV_VAR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    594\u001b[0m     )\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(WANDB_GROUP_ENV_VAR):\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(WANDB_GROUP_ENV_VAR)\n",
      "\u001b[0;31mValueError\u001b[0m: Please pass the project name as argument or through the WANDB_PROJECT_NAME environment variable."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from glob import glob\n",
    "from os import path\n",
    "\n",
    "import ray\n",
    "from ray.tune import Trainable, Tuner\n",
    "from ray.tune.registry import register_trainable, validate_trainable\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "\n",
    "from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "\n",
    "parser = add_rllib_example_script_args(\n",
    "    default_iters=10,\n",
    "    default_timesteps=1000000,\n",
    "    default_reward=300,\n",
    ")\n",
    "args = parser.parse_args(args=[])\n",
    "args.num_env_runners = 10\n",
    "args.env = 'waterworld'\n",
    "args.algo = 'PPO'\n",
    "args.num_agents = 4\n",
    "args.test_agents = 4\n",
    "\n",
    "checkpoint_path = f\"/root/test/{args.env}/{args.algo}/{args.num_agents}_agent\"\n",
    "\n",
    "sup = sorted(glob(checkpoint_path+'/*'))[0]\n",
    "\n",
    "pols = glob(sup+\"/policies/*\")\n",
    "specs = {path.basename(p) : Policy.from_checkpoint(p) for p in pols}\n",
    "\n",
    "register_env(f\"{args.num_agents}_agent_env\", lambda _: ParallelPettingZooEnv(waterworld_v4.parallel_env(n_pursuers=args.num_agents)))\n",
    "policies = {f\"pursuer_{i}\" for i in range(args.num_agents)}\n",
    "\n",
    "\n",
    "resto_config = (\n",
    "    get_trainable_cls(\"PPO\")\n",
    "    .get_default_config()\n",
    "    .environment(f\"{args.num_agents}_agent_env\")\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=(lambda aid, *args, **kwargs: aid),\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=MultiRLModuleSpec(\n",
    "            rl_module_specs={p: RLModuleSpec() for p in policies},\n",
    "        ),\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback, setup_wandb\n",
    "\n",
    "class WandbCallbackWrapper(DefaultCallbacks,WandbLoggerCallback):\n",
    "    \"\"\" \"\"\"\n",
    "    # setup(self, *args, **kwargs)\n",
    "    # log_trial_start(self)\n",
    "    # log_trial_result(self)\n",
    "    # log_trial_end(self)\n",
    "    # on_experiment_end(self)\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        DefaultCallbacks.__init__(self, **kwargs)\n",
    "        WandbLoggerCallback.__init__(self, **kwargs)\n",
    "        self.setup(self, *args, **kwargs)\n",
    "\n",
    "    def on_episode_start(self):\n",
    "        self.log_trial_start(self)\n",
    "\n",
    "    def on_episode_end(self):\n",
    "        self.log_trial_end(self)\n",
    "\n",
    "    def on_train_result(self):\n",
    "        self.log_trial_result(self)\n",
    "\n",
    "    #self.on_experiment_end(self)\n",
    "\n",
    "\n",
    "resto_algo = resto_config.callbacks(\n",
    "    WandbCallbackWrapper\n",
    ")\n",
    "\n",
    "args.wandb_project='delete_me'\n",
    "args.wandb_key='913528a8e92bf601b6eb055a459bcc89130c7f5f'\n",
    "conf={}\n",
    "\n",
    "setup_wandb(conf, args.wandb_key, project=args.wandb_project)\n",
    "\n",
    "resto_algo = resto_config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 03:33:52,868\tWARNING algorithm_config.py:4415 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "/opt/conda/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:568: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-11-05 03:33:56,751\tWARNING algorithm_config.py:4415 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "2024-11-05 03:33:56,857\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from glob import glob\n",
    "from os import path\n",
    "\n",
    "import ray\n",
    "from ray.tune import Trainable, Tuner\n",
    "from ray.tune.registry import register_trainable, validate_trainable\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "\n",
    "from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "\n",
    "parser = add_rllib_example_script_args(\n",
    "    default_iters=10,\n",
    "    default_timesteps=1000000,\n",
    "    default_reward=300,\n",
    ")\n",
    "args = parser.parse_args(args=[])\n",
    "args.num_env_runners = 10\n",
    "args.env = 'waterworld'\n",
    "args.algo = 'PPO'\n",
    "args.num_agents = 4\n",
    "args.test_agents = 4\n",
    "\n",
    "checkpoint_path = f\"/root/test/{args.env}/{args.algo}/{args.num_agents}_agent\"\n",
    "\n",
    "sup = sorted(glob(checkpoint_path+'/*'))[0]\n",
    "\n",
    "pols = glob(sup+\"/policies/*\")\n",
    "specs = {path.basename(p) : Policy.from_checkpoint(p) for p in pols}\n",
    "\n",
    "register_env(f\"{args.num_agents}_agent_env\", lambda _: ParallelPettingZooEnv(waterworld_v4.parallel_env(n_pursuers=args.num_agents)))\n",
    "policies = {f\"pursuer_{i}\" for i in range(args.num_agents)}\n",
    "\n",
    "\n",
    "resto_config = (\n",
    "    get_trainable_cls(\"PPO\")\n",
    "    .get_default_config()\n",
    "    .environment(f\"{args.num_agents}_agent_env\")\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        policy_mapping_fn=(lambda aid, *args, **kwargs: aid),\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=MultiRLModuleSpec(\n",
    "            rl_module_specs={p: RLModuleSpec() for p in policies},\n",
    "        ),\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "    )\n",
    ")\n",
    "resto_algo = resto_config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.utils import flatten_dict\n",
    "from ray.air.integrations.wandb import _is_allowed_type\n",
    "\n",
    "result = resto_algo.train()\n",
    "\n",
    "flat_result = flatten_dict(result, delimiter=\"/\")\n",
    "log = {}\n",
    "for k, v in flat_result.items():\n",
    "    log[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['num_healthy_workers', 'num_in_flight_async_sample_reqs', 'num_remote_worker_restarts', 'num_agent_steps_sampled', 'num_agent_steps_trained', 'num_env_steps_sampled', 'num_env_steps_trained', 'num_env_steps_sampled_this_iter', 'num_env_steps_trained_this_iter', 'num_env_steps_sampled_throughput_per_sec', 'num_env_steps_trained_throughput_per_sec', 'timesteps_total', 'num_env_steps_sampled_lifetime', 'num_agent_steps_sampled_lifetime', 'num_steps_trained_this_iter', 'agent_timesteps_total', 'done', 'training_iteration', 'trial_id', 'date', 'timestamp', 'time_this_iter_s', 'time_total_s', 'pid', 'hostname', 'node_ip', 'time_since_restore', 'iterations_since_restore', 'evaluation/num_agent_steps_sampled_this_iter', 'evaluation/num_env_steps_sampled_this_iter', 'evaluation/timesteps_this_iter', 'evaluation/num_healthy_workers', 'evaluation/num_in_flight_async_reqs', 'evaluation/num_remote_worker_restarts', 'info/num_env_steps_sampled', 'info/num_env_steps_trained', 'info/num_agent_steps_sampled', 'info/num_agent_steps_trained', 'info/num_env_steps_sampled_for_evaluation_this_iter', 'env_runners/episode_reward_max', 'env_runners/episode_reward_min', 'env_runners/episode_reward_mean', 'env_runners/episode_len_mean', 'env_runners/episodes_timesteps_total', 'env_runners/num_faulty_episodes', 'env_runners/num_episodes', 'env_runners/episode_return_max', 'env_runners/episode_return_min', 'env_runners/episode_return_mean', 'env_runners/episodes_this_iter', 'timers/training_iteration_time_ms', 'timers/restore_workers_time_ms', 'timers/training_step_time_ms', 'timers/sample_time_ms', 'timers/learn_time_ms', 'timers/learn_throughput', 'timers/synch_weights_time_ms', 'timers/restore_eval_workers_time_ms', 'timers/evaluation_iteration_time_ms', 'timers/evaluation_iteration_throughput', 'counters/num_env_steps_sampled', 'counters/num_env_steps_trained', 'counters/num_agent_steps_sampled', 'counters/num_agent_steps_trained', 'counters/num_env_steps_sampled_for_evaluation_this_iter', 'config/placement_strategy', 'config/num_gpus', 'config/_fake_gpus', 'config/num_cpus_for_main_process', 'config/eager_tracing', 'config/eager_max_retraces', 'config/torch_compile_learner', 'config/torch_compile_learner_what_to_compile', 'config/torch_compile_learner_dynamo_backend', 'config/torch_compile_learner_dynamo_mode', 'config/torch_compile_worker', 'config/torch_compile_worker_dynamo_backend', 'config/torch_compile_worker_dynamo_mode', 'config/torch_skip_nan_gradients', 'config/enable_rl_module_and_learner', 'config/enable_env_runner_and_connector_v2', 'config/env', 'config/observation_space', 'config/action_space', 'config/clip_rewards', 'config/normalize_actions', 'config/clip_actions', 'config/_is_atari', 'config/disable_env_checking', 'config/env_task_fn', 'config/render_env', 'config/action_mask_key', 'config/env_runner_cls', 'config/num_env_runners', 'config/num_envs_per_env_runner', 'config/num_cpus_per_env_runner', 'config/num_gpus_per_env_runner', 'config/validate_env_runners_after_construction', 'config/max_requests_in_flight_per_env_runner', 'config/sample_timeout_s', 'config/_env_to_module_connector', 'config/add_default_connectors_to_env_to_module_pipeline', 'config/_module_to_env_connector', 'config/add_default_connectors_to_module_to_env_pipeline', 'config/episode_lookback_horizon', 'config/rollout_fragment_length', 'config/batch_mode', 'config/compress_observations', 'config/remote_worker_envs', 'config/remote_env_batch_wait_ms', 'config/enable_tf1_exec_eagerly', 'config/sample_collector', 'config/preprocessor_pref', 'config/observation_filter', 'config/update_worker_filter_stats', 'config/use_worker_filter_stats', 'config/enable_connectors', 'config/sampler_perf_stats_ema_coef', 'config/num_learners', 'config/num_gpus_per_learner', 'config/num_cpus_per_learner', 'config/local_gpu_idx', 'config/gamma', 'config/lr', 'config/grad_clip', 'config/grad_clip_by', 'config/train_batch_size_per_learner', 'config/train_batch_size', 'config/num_epochs', 'config/minibatch_size', 'config/shuffle_batch_per_epoch', 'config/_learner_connector', 'config/add_default_connectors_to_learner_pipeline', 'config/_learner_class', 'config/explore', 'config/count_steps_by', 'config/policy_map_capacity', 'config/policy_mapping_fn', 'config/policies_to_train', 'config/policy_states_are_swappable', 'config/observation_fn', 'config/input_read_method', 'config/input_read_episodes', 'config/input_read_sample_batches', 'config/input_filesystem', 'config/input_compress_columns', 'config/input_spaces_jsonable', 'config/materialize_data', 'config/materialize_mapped_data', 'config/prelearner_class', 'config/prelearner_buffer_class', 'config/prelearner_module_synch_period', 'config/dataset_num_iters_per_learner', 'config/actions_in_input_normalized', 'config/postprocess_inputs', 'config/shuffle_buffer_size', 'config/output', 'config/output_compress_columns', 'config/output_max_file_size', 'config/output_max_rows_per_file', 'config/output_write_method', 'config/output_filesystem', 'config/output_write_episodes', 'config/offline_sampling', 'config/evaluation_interval', 'config/evaluation_duration', 'config/evaluation_duration_unit', 'config/evaluation_sample_timeout_s', 'config/evaluation_parallel_to_training', 'config/evaluation_force_reset_envs_before_iteration', 'config/evaluation_config', 'config/ope_split_batch_by_episode', 'config/evaluation_num_env_runners', 'config/in_evaluation', 'config/sync_filters_on_rollout_workers_timeout_s', 'config/keep_per_episode_custom_metrics', 'config/metrics_episode_collection_timeout_s', 'config/metrics_num_episodes_for_smoothing', 'config/min_time_s_per_iteration', 'config/min_train_timesteps_per_iteration', 'config/min_sample_timesteps_per_iteration', 'config/log_gradients', 'config/export_native_model_files', 'config/checkpoint_trainable_policies_only', 'config/logger_creator', 'config/logger_config', 'config/log_level', 'config/log_sys_usage', 'config/fake_sampler', 'config/seed', 'config/_run_training_always_in_thread', 'config/_evaluation_parallel_to_training_wo_thread', 'config/recreate_failed_env_runners', 'config/ignore_env_runner_failures', 'config/max_num_env_runner_restarts', 'config/delay_between_env_runner_restarts_s', 'config/restart_failed_sub_environments', 'config/num_consecutive_env_runner_failures_tolerance', 'config/env_runner_health_probe_timeout_s', 'config/env_runner_restore_timeout_s', 'config/_rl_module_spec', 'config/_AlgorithmConfig__prior_exploration_config', 'config/_torch_grad_scaler_class', 'config/_torch_lr_scheduler_classes', 'config/_tf_policy_handles_more_than_one_loss', 'config/_disable_preprocessor_api', 'config/_disable_action_flattening', 'config/_disable_initialize_loss_from_dummy_batch', 'config/_dont_auto_sync_env_runner_states', 'config/simple_optimizer', 'config/policy_map_cache', 'config/worker_cls', 'config/synchronize_filters', 'config/enable_async_evaluation', 'config/custom_async_evaluation_function', 'config/_enable_rl_module_api', 'config/auto_wrap_old_gym_envs', 'config/always_attach_evaluation_results', 'config/replay_sequence_length', 'config/_disable_execution_plan_api', 'config/lr_schedule', 'config/use_critic', 'config/use_gae', 'config/use_kl_loss', 'config/kl_coeff', 'config/kl_target', 'config/vf_loss_coeff', 'config/entropy_coeff', 'config/entropy_coeff_schedule', 'config/clip_param', 'config/vf_clip_param', 'config/sgd_minibatch_size', 'config/vf_share_layers', 'config/class', 'config/lambda', 'config/input', 'config/callbacks', 'config/create_env_on_driver', 'config/custom_eval_function', 'config/framework', 'perf/cpu_util_percent', 'perf/ram_util_percent', 'evaluation/env_runners/episode_reward_max', 'evaluation/env_runners/episode_reward_min', 'evaluation/env_runners/episode_reward_mean', 'evaluation/env_runners/episode_len_mean', 'evaluation/env_runners/episodes_timesteps_total', 'evaluation/env_runners/num_faulty_episodes', 'evaluation/env_runners/num_episodes', 'evaluation/env_runners/episode_return_max', 'evaluation/env_runners/episode_return_min', 'evaluation/env_runners/episode_return_mean', 'evaluation/env_runners/episodes_this_iter', 'env_runners/policy_reward_min/pursuer_0', 'env_runners/policy_reward_min/pursuer_1', 'env_runners/policy_reward_min/pursuer_2', 'env_runners/policy_reward_min/pursuer_3', 'env_runners/policy_reward_max/pursuer_0', 'env_runners/policy_reward_max/pursuer_1', 'env_runners/policy_reward_max/pursuer_2', 'env_runners/policy_reward_max/pursuer_3', 'env_runners/policy_reward_mean/pursuer_0', 'env_runners/policy_reward_mean/pursuer_1', 'env_runners/policy_reward_mean/pursuer_2', 'env_runners/policy_reward_mean/pursuer_3', 'env_runners/hist_stats/episode_reward', 'env_runners/hist_stats/episode_lengths', 'env_runners/hist_stats/policy_pursuer_0_reward', 'env_runners/hist_stats/policy_pursuer_1_reward', 'env_runners/hist_stats/policy_pursuer_2_reward', 'env_runners/hist_stats/policy_pursuer_3_reward', 'env_runners/sampler_perf/mean_raw_obs_processing_ms', 'env_runners/sampler_perf/mean_inference_ms', 'env_runners/sampler_perf/mean_action_processing_ms', 'env_runners/sampler_perf/mean_env_wait_ms', 'env_runners/sampler_perf/mean_env_render_ms', 'env_runners/connector_metrics/ObsPreprocessorConnector_ms', 'env_runners/connector_metrics/StateBufferConnector_ms', 'env_runners/connector_metrics/ViewRequirementAgentConnector_ms', 'config/tf_session_args/intra_op_parallelism_threads', 'config/tf_session_args/inter_op_parallelism_threads', 'config/tf_session_args/log_device_placement', 'config/tf_session_args/allow_soft_placement', 'config/local_tf_session_args/intra_op_parallelism_threads', 'config/local_tf_session_args/inter_op_parallelism_threads', 'config/model/fcnet_hiddens', 'config/model/fcnet_activation', 'config/model/fcnet_weights_initializer', 'config/model/fcnet_weights_initializer_config', 'config/model/fcnet_bias_initializer', 'config/model/fcnet_bias_initializer_config', 'config/model/conv_filters', 'config/model/conv_activation', 'config/model/conv_kernel_initializer', 'config/model/conv_kernel_initializer_config', 'config/model/conv_bias_initializer', 'config/model/conv_bias_initializer_config', 'config/model/conv_transpose_kernel_initializer', 'config/model/conv_transpose_kernel_initializer_config', 'config/model/conv_transpose_bias_initializer', 'config/model/conv_transpose_bias_initializer_config', 'config/model/post_fcnet_hiddens', 'config/model/post_fcnet_activation', 'config/model/post_fcnet_weights_initializer', 'config/model/post_fcnet_weights_initializer_config', 'config/model/post_fcnet_bias_initializer', 'config/model/post_fcnet_bias_initializer_config', 'config/model/free_log_std', 'config/model/log_std_clip_param', 'config/model/no_final_linear', 'config/model/vf_share_layers', 'config/model/use_lstm', 'config/model/max_seq_len', 'config/model/lstm_cell_size', 'config/model/lstm_use_prev_action', 'config/model/lstm_use_prev_reward', 'config/model/lstm_weights_initializer', 'config/model/lstm_weights_initializer_config', 'config/model/lstm_bias_initializer', 'config/model/lstm_bias_initializer_config', 'config/model/_time_major', 'config/model/use_attention', 'config/model/attention_num_transformer_units', 'config/model/attention_dim', 'config/model/attention_num_heads', 'config/model/attention_head_dim', 'config/model/attention_memory_inference', 'config/model/attention_memory_training', 'config/model/attention_position_wise_mlp_dim', 'config/model/attention_init_gru_gate_bias', 'config/model/attention_use_n_prev_actions', 'config/model/attention_use_n_prev_rewards', 'config/model/framestack', 'config/model/dim', 'config/model/grayscale', 'config/model/zero_mean', 'config/model/custom_model', 'config/model/custom_action_dist', 'config/model/custom_preprocessor', 'config/model/encoder_latent_dim', 'config/model/always_check_shapes', 'config/model/lstm_use_prev_action_reward', 'config/model/_use_default_native_models', 'config/model/_disable_preprocessor_api', 'config/model/_disable_action_flattening', 'config/exploration_config/type', 'config/policies/pursuer_1', 'config/policies/pursuer_0', 'config/policies/pursuer_3', 'config/policies/pursuer_2', 'evaluation/env_runners/policy_reward_min/pursuer_0', 'evaluation/env_runners/policy_reward_min/pursuer_1', 'evaluation/env_runners/policy_reward_min/pursuer_2', 'evaluation/env_runners/policy_reward_min/pursuer_3', 'evaluation/env_runners/policy_reward_max/pursuer_0', 'evaluation/env_runners/policy_reward_max/pursuer_1', 'evaluation/env_runners/policy_reward_max/pursuer_2', 'evaluation/env_runners/policy_reward_max/pursuer_3', 'evaluation/env_runners/policy_reward_mean/pursuer_0', 'evaluation/env_runners/policy_reward_mean/pursuer_1', 'evaluation/env_runners/policy_reward_mean/pursuer_2', 'evaluation/env_runners/policy_reward_mean/pursuer_3', 'evaluation/env_runners/hist_stats/episode_reward', 'evaluation/env_runners/hist_stats/episode_lengths', 'evaluation/env_runners/hist_stats/policy_pursuer_0_reward', 'evaluation/env_runners/hist_stats/policy_pursuer_1_reward', 'evaluation/env_runners/hist_stats/policy_pursuer_2_reward', 'evaluation/env_runners/hist_stats/policy_pursuer_3_reward', 'evaluation/env_runners/sampler_perf/mean_raw_obs_processing_ms', 'evaluation/env_runners/sampler_perf/mean_inference_ms', 'evaluation/env_runners/sampler_perf/mean_action_processing_ms', 'evaluation/env_runners/sampler_perf/mean_env_wait_ms', 'evaluation/env_runners/sampler_perf/mean_env_render_ms', 'evaluation/env_runners/connector_metrics/ObsPreprocessorConnector_ms', 'evaluation/env_runners/connector_metrics/StateBufferConnector_ms', 'evaluation/env_runners/connector_metrics/ViewRequirementAgentConnector_ms', 'info/learner/pursuer_2/num_agent_steps_trained', 'info/learner/pursuer_2/num_grad_updates_lifetime', 'info/learner/pursuer_2/diff_num_grad_updates_vs_sampler_policy', 'info/learner/pursuer_1/num_agent_steps_trained', 'info/learner/pursuer_1/num_grad_updates_lifetime', 'info/learner/pursuer_1/diff_num_grad_updates_vs_sampler_policy', 'info/learner/pursuer_0/num_agent_steps_trained', 'info/learner/pursuer_0/num_grad_updates_lifetime', 'info/learner/pursuer_0/diff_num_grad_updates_vs_sampler_policy', 'info/learner/pursuer_3/num_agent_steps_trained', 'info/learner/pursuer_3/num_grad_updates_lifetime', 'info/learner/pursuer_3/diff_num_grad_updates_vs_sampler_policy', 'config/tf_session_args/gpu_options/allow_growth', 'config/tf_session_args/device_count/CPU', 'info/learner/pursuer_2/learner_stats/allreduce_latency', 'info/learner/pursuer_2/learner_stats/grad_gnorm', 'info/learner/pursuer_2/learner_stats/cur_kl_coeff', 'info/learner/pursuer_2/learner_stats/cur_lr', 'info/learner/pursuer_2/learner_stats/total_loss', 'info/learner/pursuer_2/learner_stats/policy_loss', 'info/learner/pursuer_2/learner_stats/vf_loss', 'info/learner/pursuer_2/learner_stats/vf_explained_var', 'info/learner/pursuer_2/learner_stats/kl', 'info/learner/pursuer_2/learner_stats/entropy', 'info/learner/pursuer_2/learner_stats/entropy_coeff', 'info/learner/pursuer_1/learner_stats/allreduce_latency', 'info/learner/pursuer_1/learner_stats/grad_gnorm', 'info/learner/pursuer_1/learner_stats/cur_kl_coeff', 'info/learner/pursuer_1/learner_stats/cur_lr', 'info/learner/pursuer_1/learner_stats/total_loss', 'info/learner/pursuer_1/learner_stats/policy_loss', 'info/learner/pursuer_1/learner_stats/vf_loss', 'info/learner/pursuer_1/learner_stats/vf_explained_var', 'info/learner/pursuer_1/learner_stats/kl', 'info/learner/pursuer_1/learner_stats/entropy', 'info/learner/pursuer_1/learner_stats/entropy_coeff', 'info/learner/pursuer_0/learner_stats/allreduce_latency', 'info/learner/pursuer_0/learner_stats/grad_gnorm', 'info/learner/pursuer_0/learner_stats/cur_kl_coeff', 'info/learner/pursuer_0/learner_stats/cur_lr', 'info/learner/pursuer_0/learner_stats/total_loss', 'info/learner/pursuer_0/learner_stats/policy_loss', 'info/learner/pursuer_0/learner_stats/vf_loss', 'info/learner/pursuer_0/learner_stats/vf_explained_var', 'info/learner/pursuer_0/learner_stats/kl', 'info/learner/pursuer_0/learner_stats/entropy', 'info/learner/pursuer_0/learner_stats/entropy_coeff', 'info/learner/pursuer_3/learner_stats/allreduce_latency', 'info/learner/pursuer_3/learner_stats/grad_gnorm', 'info/learner/pursuer_3/learner_stats/cur_kl_coeff', 'info/learner/pursuer_3/learner_stats/cur_lr', 'info/learner/pursuer_3/learner_stats/total_loss', 'info/learner/pursuer_3/learner_stats/policy_loss', 'info/learner/pursuer_3/learner_stats/vf_loss', 'info/learner/pursuer_3/learner_stats/vf_explained_var', 'info/learner/pursuer_3/learner_stats/kl', 'info/learner/pursuer_3/learner_stats/entropy', 'info/learner/pursuer_3/learner_stats/entropy_coeff'])\n"
     ]
    }
   ],
   "source": [
    "print(log.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
