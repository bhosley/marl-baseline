{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ray[rllib,tune]\n",
    "#!pip install pettingzoo pygame pymunk\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "from ray.rllib.core.rl_module.marl_module import MultiAgentRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv, ParallelPettingZooEnv\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "\n",
    "from pettingzoo.sisl import waterworld_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 02:06:20,467\tWARNING algorithm_config.py:4258 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "/opt/conda/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:557: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-08-29 02:06:21,596\tWARNING services.py:2017 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67084288 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-08-29 02:06:21,682\tINFO worker.py:1772 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2024-08-29 02:06:26,362\tWARNING algorithm_config.py:4258 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "2024-08-29 02:06:26,405\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from glob import glob\n",
    "from os import path\n",
    "from ray.rllib.policy.policy import Policy\n",
    "\n",
    "ray.shutdown() \n",
    "\n",
    "# Trained to about 0 combined return\n",
    "checkpoint_path = \"/root/ray_results/PPO_2024-08-28_20-57-45/PPO_2_agent_env_2cf59_00000_0_2024-08-28_20-57-45/checkpoint_000000\"\n",
    "pols = glob(checkpoint_path+\"/policies/*\")\n",
    "specs = {path.basename(p) : Policy.from_checkpoint(p) for p in pols}\n",
    "#specs = {path.basename(p) : SingleAgentRLModuleSpec(load_state_path=p) for p in pols} # Non-deterministic policy weight return (implies new)\n",
    "\n",
    "\n",
    "num_agents = 2\n",
    "\n",
    "register_env(f\"{num_agents}_agent_env\", lambda _: ParallelPettingZooEnv(waterworld_v4.parallel_env(n_pursuers=num_agents)))\n",
    "policies = {f\"pursuer_{i}\" for i in range(num_agents)}\n",
    "\n",
    "\n",
    "resto_config = (\n",
    "    get_trainable_cls(\"PPO\")\n",
    "    .get_default_config()\n",
    "    .environment(f\"{num_agents}_agent_env\")\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        # Exact 1:1 mapping from AgentID to ModuleID.\n",
    "        policy_mapping_fn=(lambda aid, *args, **kwargs: aid),\n",
    "    )\n",
    "    .rl_module(\n",
    "        #model_config_dict={\"vf_share_layers\": True},\n",
    "        rl_module_spec=MultiAgentRLModuleSpec(\n",
    "            #load_state_path=\n",
    "            #module_specs=specs,\n",
    "            module_specs={p: SingleAgentRLModuleSpec() for p in policies},\n",
    "        ),\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "resto_algo = resto_config.build()\n",
    "resto_algo.get_policy(\"pursuer_0\").set_weights(specs[\"pursuer_0\"].get_weights())\n",
    "resto_algo.get_policy(\"pursuer_1\").set_weights(specs[\"pursuer_1\"].get_weights())\n",
    "#resto_algo.get_policy(\"pursuer_0\").get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we restore an algo to its original size, and evaluate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env_runners': {'episode_reward_max': np.float64(104.48447046235398),\n",
       "  'episode_reward_min': np.float64(-49.97758701259906),\n",
       "  'episode_reward_mean': np.float64(34.31495169008534),\n",
       "  'episode_len_mean': np.float64(500.0),\n",
       "  'episode_media': {},\n",
       "  'episodes_timesteps_total': 5000,\n",
       "  'policy_reward_min': {'pursuer_0': np.float64(-27.80399612520721),\n",
       "   'pursuer_1': np.float64(-74.73581968384396)},\n",
       "  'policy_reward_max': {'pursuer_0': np.float64(108.50397211584374),\n",
       "   'pursuer_1': np.float64(66.59594375510522)},\n",
       "  'policy_reward_mean': {'pursuer_0': np.float64(32.97170784631929),\n",
       "   'pursuer_1': np.float64(1.3432438437660394)},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [np.float64(44.487376281462424),\n",
       "    np.float64(98.65380257932057),\n",
       "    np.float64(84.50964155764572),\n",
       "    np.float64(20.08502968193733),\n",
       "    np.float64(104.48447046235398),\n",
       "    np.float64(-38.577462399422544),\n",
       "    np.float64(-49.97758701259906),\n",
       "    np.float64(20.465628189378837),\n",
       "    np.float64(91.2633147856224),\n",
       "    np.float64(-32.24469722484628)],\n",
       "   'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500],\n",
       "   'policy_pursuer_0_reward': [np.float64(40.07564281289912),\n",
       "    np.float64(53.608308921809055),\n",
       "    np.float64(70.12441925264447),\n",
       "    np.float64(-27.80399612520721),\n",
       "    np.float64(37.88852670724881),\n",
       "    np.float64(-4.464299391091167),\n",
       "    np.float64(4.619717553032),\n",
       "    np.float64(4.673664157016454),\n",
       "    np.float64(108.50397211584374),\n",
       "    np.float64(42.49112245899767)],\n",
       "   'policy_pursuer_1_reward': [np.float64(4.411733468563258),\n",
       "    np.float64(45.04549365751143),\n",
       "    np.float64(14.38522230500113),\n",
       "    np.float64(47.889025807144655),\n",
       "    np.float64(66.59594375510522),\n",
       "    np.float64(-34.113163008331284),\n",
       "    np.float64(-54.59730456563125),\n",
       "    np.float64(15.79196403236251),\n",
       "    np.float64(-17.24065733022132),\n",
       "    np.float64(-74.73581968384396)]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(0.337330916680976),\n",
       "   'mean_inference_ms': np.float64(1.317882094427581),\n",
       "   'mean_action_processing_ms': np.float64(0.18205562599562794),\n",
       "   'mean_env_wait_ms': np.float64(1.6657234775866285),\n",
       "   'mean_env_render_ms': np.float64(0.0)},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.0024259090423583984),\n",
       "   'StateBufferConnector_ms': np.float64(0.0023412704467773438),\n",
       "   'ViewRequirementAgentConnector_ms': np.float64(0.05772590637207031)},\n",
       "  'num_episodes': 10,\n",
       "  'episode_return_max': np.float64(104.48447046235398),\n",
       "  'episode_return_min': np.float64(-49.97758701259906),\n",
       "  'episode_return_mean': np.float64(34.31495169008534),\n",
       "  'episodes_this_iter': 10},\n",
       " 'num_agent_steps_sampled_this_iter': 10000,\n",
       " 'num_env_steps_sampled_this_iter': 5000,\n",
       " 'timesteps_this_iter': 5000}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resto_algo.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets try with a different number of test agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/opt/conda/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2024-09-03 17:50:57,409\tWARNING algorithm_config.py:4258 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "/opt/conda/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:557: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-09-03 17:50:58,981\tWARNING services.py:2017 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67059712 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-09-03 17:51:00,086\tINFO worker.py:1772 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "2024-09-03 17:51:06,518\tWARNING algorithm_config.py:4258 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "2024-09-03 17:51:06,899\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'env_runners': {'episode_reward_max': np.float64(-133.08651258384768),\n",
       "  'episode_reward_min': np.float64(-429.6043485772751),\n",
       "  'episode_reward_mean': np.float64(-279.01460767396213),\n",
       "  'episode_len_mean': np.float64(500.0),\n",
       "  'episode_media': {},\n",
       "  'episodes_timesteps_total': 5000,\n",
       "  'policy_reward_min': {'pursuer_0': np.float64(-70.69341337433875),\n",
       "   'pursuer_1': np.float64(-83.28156152153042),\n",
       "   'pursuer_2': np.float64(-89.37824978401237),\n",
       "   'pursuer_3': np.float64(-64.26033845731828),\n",
       "   'pursuer_4': np.float64(-104.50749893251731),\n",
       "   'pursuer_5': np.float64(-180.21766134359572)},\n",
       "  'policy_reward_max': {'pursuer_0': np.float64(-8.571273725909617),\n",
       "   'pursuer_1': np.float64(60.30311215169101),\n",
       "   'pursuer_2': np.float64(42.74708175708947),\n",
       "   'pursuer_3': np.float64(-7.6108996017414645),\n",
       "   'pursuer_4': np.float64(67.86120096904311),\n",
       "   'pursuer_5': np.float64(-44.10802049107301)},\n",
       "  'policy_reward_mean': {'pursuer_0': np.float64(-42.284014583047856),\n",
       "   'pursuer_1': np.float64(-40.1377328008583),\n",
       "   'pursuer_2': np.float64(-40.48262924096856),\n",
       "   'pursuer_3': np.float64(-45.66979861051086),\n",
       "   'pursuer_4': np.float64(1.1955199344886225),\n",
       "   'pursuer_5': np.float64(-111.63595237306478)},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [np.float64(-166.19797254025622),\n",
       "    np.float64(-326.0186364467571),\n",
       "    np.float64(-345.568169571401),\n",
       "    np.float64(-429.6043485772751),\n",
       "    np.float64(-220.67587333793387),\n",
       "    np.float64(-185.92611986327765),\n",
       "    np.float64(-262.3994365645389),\n",
       "    np.float64(-415.2977044772379),\n",
       "    np.float64(-133.08651258384768),\n",
       "    np.float64(-305.3713027770962)],\n",
       "   'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500],\n",
       "   'policy_pursuer_0_reward': [np.float64(-19.490907228469617),\n",
       "    np.float64(-41.93875258924693),\n",
       "    np.float64(-8.571273725909617),\n",
       "    np.float64(-70.69341337433875),\n",
       "    np.float64(-36.23102767416677),\n",
       "    np.float64(-53.02094782426345),\n",
       "    np.float64(-40.14230482430377),\n",
       "    np.float64(-53.63067677345778),\n",
       "    np.float64(-58.96140226997416),\n",
       "    np.float64(-40.1594395463477)],\n",
       "   'policy_pursuer_1_reward': [np.float64(-20.32164907733235),\n",
       "    np.float64(-63.36892477087198),\n",
       "    np.float64(-78.8616327184739),\n",
       "    np.float64(-83.28156152153042),\n",
       "    np.float64(-17.192987182804654),\n",
       "    np.float64(60.30311215169101),\n",
       "    np.float64(-81.58622316165038),\n",
       "    np.float64(15.66510774069288),\n",
       "    np.float64(-59.336612818969634),\n",
       "    np.float64(-73.39595664933358)],\n",
       "   'policy_pursuer_2_reward': [np.float64(42.74708175708947),\n",
       "    np.float64(-48.91667205879994),\n",
       "    np.float64(-60.285853924302046),\n",
       "    np.float64(-75.61061275050122),\n",
       "    np.float64(-50.681475798239205),\n",
       "    np.float64(-28.212178049285676),\n",
       "    np.float64(-24.130360475552504),\n",
       "    np.float64(-89.37824978401237),\n",
       "    np.float64(-13.983339211166555),\n",
       "    np.float64(-56.37463211491561)],\n",
       "   'policy_pursuer_3_reward': [np.float64(-56.420543595035035),\n",
       "    np.float64(-30.623046206764013),\n",
       "    np.float64(-57.08194477161877),\n",
       "    np.float64(-51.94511833327789),\n",
       "    np.float64(-33.07761992343081),\n",
       "    np.float64(-64.26033845731828),\n",
       "    np.float64(-35.91195257054746),\n",
       "    np.float64(-61.13473613175747),\n",
       "    np.float64(-7.6108996017414645),\n",
       "    np.float64(-58.631786513617456)],\n",
       "   'policy_pursuer_4_reward': [np.float64(-33.06575598603719),\n",
       "    np.float64(-3.5606353837811464),\n",
       "    np.float64(39.45019691249885),\n",
       "    np.float64(29.81497359985701),\n",
       "    np.float64(-9.630923310699172),\n",
       "    np.float64(9.37285418175031),\n",
       "    np.float64(48.922254756580564),\n",
       "    np.float64(-104.50749893251731),\n",
       "    np.float64(67.86120096904311),\n",
       "    np.float64(-32.70146746180881)],\n",
       "   'policy_pursuer_5_reward': [np.float64(-79.64619841047161),\n",
       "    np.float64(-137.61060543729332),\n",
       "    np.float64(-180.21766134359572),\n",
       "    np.float64(-177.88861619748133),\n",
       "    np.float64(-73.86183944859376),\n",
       "    np.float64(-110.1086218658516),\n",
       "    np.float64(-129.55085028906458),\n",
       "    np.float64(-122.31165059618439),\n",
       "    np.float64(-61.05545965103868),\n",
       "    np.float64(-44.10802049107301)]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(1.183615472645217),\n",
       "   'mean_inference_ms': np.float64(5.674096303709839),\n",
       "   'mean_action_processing_ms': np.float64(0.6120333169083957),\n",
       "   'mean_env_wait_ms': np.float64(6.530168032174205),\n",
       "   'mean_env_render_ms': np.float64(0.0)},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.005753835042317708),\n",
       "   'StateBufferConnector_ms': np.float64(0.002828836441040039),\n",
       "   'ViewRequirementAgentConnector_ms': np.float64(0.07375001907348633)},\n",
       "  'num_episodes': 10,\n",
       "  'episode_return_max': np.float64(-133.08651258384768),\n",
       "  'episode_return_min': np.float64(-429.6043485772751),\n",
       "  'episode_return_mean': np.float64(-279.01460767396213),\n",
       "  'episodes_this_iter': 10},\n",
       " 'num_agent_steps_sampled_this_iter': 30000,\n",
       " 'num_env_steps_sampled_this_iter': 5000,\n",
       " 'timesteps_this_iter': 5000}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from glob import glob\n",
    "from os import path\n",
    "from ray.rllib.policy.policy import Policy\n",
    "import numpy as np\n",
    "\n",
    "ray.shutdown() \n",
    "\n",
    "# Trained to about 0 combined return\n",
    "checkpoint_path = \"/root/ray_results/PPO_2024-08-28_20-57-45/PPO_2_agent_env_2cf59_00000_0_2024-08-28_20-57-45/checkpoint_000000\"\n",
    "pols = glob(checkpoint_path+\"/policies/*\")\n",
    "specs = {path.basename(p) : Policy.from_checkpoint(p) for p in pols}\n",
    "#specs = {path.basename(p) : SingleAgentRLModuleSpec(load_state_path=p) for p in pols} # Non-deterministic policy weight return (implies new)\n",
    "\n",
    "num_trained_agents = 2\n",
    "num_test_agents = 6\n",
    "\n",
    "register_env(f\"{num_test_agents}_agent_env\", lambda _: ParallelPettingZooEnv(waterworld_v4.parallel_env(n_pursuers=num_test_agents)))\n",
    "policies = {f\"pursuer_{i}\" for i in range(num_test_agents)}\n",
    "\n",
    "\n",
    "resto_config = (\n",
    "    get_trainable_cls(\"PPO\")\n",
    "    .get_default_config()\n",
    "    .environment(f\"{num_test_agents}_agent_env\")\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        # Exact 1:1 mapping from AgentID to ModuleID.\n",
    "        policy_mapping_fn=(lambda aid, *args, **kwargs: aid),\n",
    "    )\n",
    "    .rl_module(\n",
    "        #model_config_dict={\"vf_share_layers\": True},\n",
    "        rl_module_spec=MultiAgentRLModuleSpec(\n",
    "            #load_state_path=\n",
    "            #module_specs=specs,\n",
    "            module_specs={p: SingleAgentRLModuleSpec() for p in policies},\n",
    "        ),\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "resto_algo = resto_config.build()\n",
    "for test_id in range(num_test_agents):\n",
    "    train_id = np.random.randint(num_trained_agents)\n",
    "    resto_algo.get_policy(f\"pursuer_{test_id}\").set_weights(specs[f\"pursuer_{train_id}\"].get_weights())\n",
    "\n",
    "#resto_algo.get_policy(\"pursuer_0\").set_weights(specs[\"pursuer_0\"].get_weights())\n",
    "#resto_algo.get_policy(\"pursuer_1\").set_weights(specs[\"pursuer_1\"].get_weights())\n",
    "#resto_algo.get_policy(\"pursuer_0\").get_weights()\n",
    "\n",
    "resto_algo.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 17:55:20,672\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'env_runners': {'episode_reward_max': np.float64(-364.769848341295),\n",
       "   'episode_reward_min': np.float64(-619.0375683355406),\n",
       "   'episode_reward_mean': np.float64(-493.78093420107905),\n",
       "   'episode_len_mean': np.float64(500.0),\n",
       "   'episode_media': {},\n",
       "   'episodes_timesteps_total': 5000,\n",
       "   'policy_reward_min': {'pursuer_0': np.float64(-113.90485384019159),\n",
       "    'pursuer_1': np.float64(-109.27251741852379),\n",
       "    'pursuer_2': np.float64(-114.55402342215696),\n",
       "    'pursuer_3': np.float64(-111.76090622319133),\n",
       "    'pursuer_4': np.float64(-109.0597747992956),\n",
       "    'pursuer_5': np.float64(-217.50166684483233)},\n",
       "   'policy_reward_max': {'pursuer_0': np.float64(11.303415129637894),\n",
       "    'pursuer_1': np.float64(0.2069972847033158),\n",
       "    'pursuer_2': np.float64(-46.84567547040106),\n",
       "    'pursuer_3': np.float64(-6.933874989835202),\n",
       "    'pursuer_4': np.float64(-7.903867561959164),\n",
       "    'pursuer_5': np.float64(-102.74657319368887)},\n",
       "   'policy_reward_mean': {'pursuer_0': np.float64(-62.68889913535977),\n",
       "    'pursuer_1': np.float64(-65.25422536988678),\n",
       "    'pursuer_2': np.float64(-74.27730941244923),\n",
       "    'pursuer_3': np.float64(-65.3008027191293),\n",
       "    'pursuer_4': np.float64(-63.75138013879174),\n",
       "    'pursuer_5': np.float64(-162.50831742546185)},\n",
       "   'custom_metrics': {},\n",
       "   'hist_stats': {'episode_reward': [np.float64(-512.3823702423662),\n",
       "     np.float64(-508.5460752882049),\n",
       "     np.float64(-613.3155297566499),\n",
       "     np.float64(-364.769848341295),\n",
       "     np.float64(-495.4410446420681),\n",
       "     np.float64(-619.0375683355406),\n",
       "     np.float64(-389.5228384962124),\n",
       "     np.float64(-382.61291184917025),\n",
       "     np.float64(-603.3023013627752),\n",
       "     np.float64(-448.8788536965084)],\n",
       "    'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500],\n",
       "    'policy_pursuer_0_reward': [np.float64(-60.37390249750134),\n",
       "     np.float64(-60.741754372675686),\n",
       "     np.float64(-92.38409091020394),\n",
       "     np.float64(11.303415129637894),\n",
       "     np.float64(-89.95122930874328),\n",
       "     np.float64(-113.90485384019159),\n",
       "     np.float64(-33.69541775768187),\n",
       "     np.float64(-57.605645430906925),\n",
       "     np.float64(-82.23981599404104),\n",
       "     np.float64(-47.29569637128995)],\n",
       "    'policy_pursuer_1_reward': [np.float64(-69.16587427966073),\n",
       "     np.float64(-49.64229550423252),\n",
       "     np.float64(-96.41777564875132),\n",
       "     np.float64(-69.87433716798475),\n",
       "     np.float64(-109.27251741852379),\n",
       "     np.float64(-89.99336501878994),\n",
       "     np.float64(0.2069972847033158),\n",
       "     np.float64(-10.887723201806109),\n",
       "     np.float64(-93.20806475836568),\n",
       "     np.float64(-64.28729798545638)],\n",
       "    'policy_pursuer_2_reward': [np.float64(-89.8094922272591),\n",
       "     np.float64(-68.80568767053518),\n",
       "     np.float64(-114.55402342215696),\n",
       "     np.float64(-70.71135157105256),\n",
       "     np.float64(-75.0275389938279),\n",
       "     np.float64(-47.789534609511726),\n",
       "     np.float64(-46.88852374743173),\n",
       "     np.float64(-91.35057806874335),\n",
       "     np.float64(-46.84567547040106),\n",
       "     np.float64(-90.99068834357274)],\n",
       "    'policy_pursuer_3_reward': [np.float64(-111.32630440205934),\n",
       "     np.float64(-103.95080333396903),\n",
       "     np.float64(-84.83800127252131),\n",
       "     np.float64(-6.933874989835202),\n",
       "     np.float64(-8.589331807829799),\n",
       "     np.float64(-111.76090622319133),\n",
       "     np.float64(-30.30373001435348),\n",
       "     np.float64(-11.583014317344874),\n",
       "     np.float64(-110.78242937291465),\n",
       "     np.float64(-72.93963145727405)],\n",
       "    'policy_pursuer_4_reward': [np.float64(-66.7274235142958),\n",
       "     np.float64(-7.903867561959164),\n",
       "     np.float64(-54.9051831086639),\n",
       "     np.float64(-109.0597747992956),\n",
       "     np.float64(-52.28148786129459),\n",
       "     np.float64(-71.4751414424136),\n",
       "     np.float64(-67.6893324307532),\n",
       "     np.float64(-46.52229002548692),\n",
       "     np.float64(-90.3303342985284),\n",
       "     np.float64(-70.61896634522623)],\n",
       "    'policy_pursuer_5_reward': [np.float64(-114.97937332158938),\n",
       "     np.float64(-217.50166684483233),\n",
       "     np.float64(-170.21645539435244),\n",
       "     np.float64(-119.49392494276402),\n",
       "     np.float64(-160.31893925184878),\n",
       "     np.float64(-184.113767201442),\n",
       "     np.float64(-211.15283183069522),\n",
       "     np.float64(-164.66366080488214),\n",
       "     np.float64(-179.89598146852364),\n",
       "     np.float64(-102.74657319368887)]},\n",
       "   'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(1.2004366875553045),\n",
       "    'mean_inference_ms': np.float64(5.637246124172983),\n",
       "    'mean_action_processing_ms': np.float64(0.6170369138623248),\n",
       "    'mean_env_wait_ms': np.float64(6.577228381745566),\n",
       "    'mean_env_render_ms': np.float64(0.0)},\n",
       "   'num_faulty_episodes': 0,\n",
       "   'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.0030291080474853516),\n",
       "    'StateBufferConnector_ms': np.float64(0.002672274907430013),\n",
       "    'ViewRequirementAgentConnector_ms': np.float64(0.0721895694732666)},\n",
       "   'num_episodes': 10,\n",
       "   'episode_return_max': np.float64(-364.769848341295),\n",
       "   'episode_return_min': np.float64(-619.0375683355406),\n",
       "   'episode_return_mean': np.float64(-493.78093420107905),\n",
       "   'episodes_this_iter': 10},\n",
       "  'num_agent_steps_sampled_this_iter': 30000,\n",
       "  'num_env_steps_sampled_this_iter': 5000,\n",
       "  'timesteps_this_iter': 5000,\n",
       "  'num_healthy_workers': 0,\n",
       "  'num_in_flight_async_reqs': 0,\n",
       "  'num_remote_worker_restarts': 0},\n",
       " 'custom_metrics': {},\n",
       " 'episode_media': {},\n",
       " 'info': {'learner': {'pursuer_1': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(8.284326),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(6.566026151676973),\n",
       "     'policy_loss': np.float64(0.09367009556881385),\n",
       "     'vf_loss': np.float64(6.283298968772093),\n",
       "     'vf_explained_var': np.float64(0.08557038928071657),\n",
       "     'kl': np.float64(0.9452853819413576),\n",
       "     'entropy': np.float64(3.36656474818786),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)},\n",
       "   'pursuer_2': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(9.113566),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(6.077735746900241),\n",
       "     'policy_loss': np.float64(0.0958497002410392),\n",
       "     'vf_loss': np.float64(5.798774990439415),\n",
       "     'vf_explained_var': np.float64(0.037562563084065914),\n",
       "     'kl': np.float64(0.9155552069811771),\n",
       "     'entropy': np.float64(3.340836751833558),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)},\n",
       "   'pursuer_4': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(5.2964835),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(6.91358299801747),\n",
       "     'policy_loss': np.float64(0.06978534823089527),\n",
       "     'vf_loss': np.float64(6.716566329697768),\n",
       "     'vf_explained_var': np.float64(-0.01836751662194729),\n",
       "     'kl': np.float64(0.6361566481510332),\n",
       "     'entropy': np.float64(3.28274431626002),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)},\n",
       "   'pursuer_3': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(10.332651),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(6.854018593331178),\n",
       "     'policy_loss': np.float64(0.09294957247087345),\n",
       "     'vf_loss': np.float64(6.589247601230939),\n",
       "     'vf_explained_var': np.float64(0.09223279201736052),\n",
       "     'kl': np.float64(0.8591070479790991),\n",
       "     'entropy': np.float64(3.249066265951842),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)},\n",
       "   'pursuer_0': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(13.473987),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(8.077613924443721),\n",
       "     'policy_loss': np.float64(0.2053523147891004),\n",
       "     'vf_loss': np.float64(7.52750322073698),\n",
       "     'vf_explained_var': np.float64(0.006246730933586756),\n",
       "     'kl': np.float64(1.723792077577673),\n",
       "     'entropy': np.float64(3.565287616228064),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)},\n",
       "   'pursuer_5': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(6.1548185),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(9.17444859991471),\n",
       "     'policy_loss': np.float64(0.06458147976275844),\n",
       "     'vf_loss': np.float64(8.96580339173476),\n",
       "     'vf_explained_var': np.float64(0.117337599893411),\n",
       "     'kl': np.float64(0.7203187775623519),\n",
       "     'entropy': np.float64(3.3291859274109203),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)}},\n",
       "  'num_env_steps_sampled_for_evaluation_this_iter': 5000,\n",
       "  'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 4000,\n",
       "  'num_agent_steps_sampled': 24000,\n",
       "  'num_agent_steps_trained': 24000},\n",
       " 'env_runners': {'episode_reward_max': np.float64(-327.1718818863369),\n",
       "  'episode_reward_min': np.float64(-651.0568312071787),\n",
       "  'episode_reward_mean': np.float64(-521.166285331625),\n",
       "  'episode_len_mean': np.float64(500.0),\n",
       "  'episode_media': {},\n",
       "  'episodes_timesteps_total': 4000,\n",
       "  'policy_reward_min': {'pursuer_0': np.float64(-89.84811267121047),\n",
       "   'pursuer_1': np.float64(-112.50249861020443),\n",
       "   'pursuer_2': np.float64(-91.24802819055824),\n",
       "   'pursuer_3': np.float64(-107.97904688389782),\n",
       "   'pursuer_4': np.float64(-111.11837925890522),\n",
       "   'pursuer_5': np.float64(-212.1694086810163)},\n",
       "  'policy_reward_max': {'pursuer_0': np.float64(-7.318217051177981),\n",
       "   'pursuer_1': np.float64(-27.58393728642566),\n",
       "   'pursuer_2': np.float64(-48.906998868469366),\n",
       "   'pursuer_3': np.float64(-9.74873670808441),\n",
       "   'pursuer_4': np.float64(-8.12314018169635),\n",
       "   'pursuer_5': np.float64(-79.59940902683455)},\n",
       "  'policy_reward_mean': {'pursuer_0': np.float64(-49.37423269147564),\n",
       "   'pursuer_1': np.float64(-78.80312803094313),\n",
       "   'pursuer_2': np.float64(-79.74707460928532),\n",
       "   'pursuer_3': np.float64(-73.11886170322728),\n",
       "   'pursuer_4': np.float64(-69.70677086543017),\n",
       "   'pursuer_5': np.float64(-170.41621743126296)},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [np.float64(-327.1718818863369),\n",
       "    np.float64(-651.0568312071787),\n",
       "    np.float64(-620.2393821320601),\n",
       "    np.float64(-489.3921927436716),\n",
       "    np.float64(-410.1731976198102),\n",
       "    np.float64(-581.8466747940897),\n",
       "    np.float64(-575.4050666625217),\n",
       "    np.float64(-514.0450556073309)],\n",
       "   'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500],\n",
       "   'policy_pursuer_0_reward': [np.float64(-7.318217051177981),\n",
       "    np.float64(-89.84811267121047),\n",
       "    np.float64(-67.81082695742018),\n",
       "    np.float64(-71.91190472880133),\n",
       "    np.float64(-27.599276463261358),\n",
       "    np.float64(-31.960898365748204),\n",
       "    np.float64(-49.030113199761296),\n",
       "    np.float64(-49.51451209442428)],\n",
       "   'policy_pursuer_1_reward': [np.float64(-74.23225189379251),\n",
       "    np.float64(-68.39240528210188),\n",
       "    np.float64(-111.25103087508597),\n",
       "    np.float64(-111.30135546528297),\n",
       "    np.float64(-78.52141808791102),\n",
       "    np.float64(-46.64012674674057),\n",
       "    np.float64(-112.50249861020443),\n",
       "    np.float64(-27.58393728642566)],\n",
       "   'policy_pursuer_2_reward': [np.float64(-48.906998868469366),\n",
       "    np.float64(-91.24802819055824),\n",
       "    np.float64(-91.00219260111501),\n",
       "    np.float64(-88.84490928958596),\n",
       "    np.float64(-68.45956923680166),\n",
       "    np.float64(-89.8016870322555),\n",
       "    np.float64(-90.06113917726304),\n",
       "    np.float64(-69.65207247823379)],\n",
       "   'policy_pursuer_3_reward': [np.float64(-9.74873670808441),\n",
       "    np.float64(-107.97904688389782),\n",
       "    np.float64(-66.036674815765),\n",
       "    np.float64(-84.80410159987245),\n",
       "    np.float64(-68.74731589259862),\n",
       "    np.float64(-90.15617470942277),\n",
       "    np.float64(-88.53004439188066),\n",
       "    np.float64(-68.94879862429651)],\n",
       "   'policy_pursuer_4_reward': [np.float64(-8.12314018169635),\n",
       "    np.float64(-90.02686706189297),\n",
       "    np.float64(-111.09759313888274),\n",
       "    np.float64(-52.930512633294214),\n",
       "    np.float64(-9.520213959131988),\n",
       "    np.float64(-111.11837925890522),\n",
       "    np.float64(-68.83816491212305),\n",
       "    np.float64(-105.99929577751489)],\n",
       "   'policy_pursuer_5_reward': [np.float64(-178.84253718311618),\n",
       "    np.float64(-203.5623711175165),\n",
       "    np.float64(-173.04106374379046),\n",
       "    np.float64(-79.59940902683455),\n",
       "    np.float64(-157.3254039801049),\n",
       "    np.float64(-212.1694086810163),\n",
       "    np.float64(-166.44310637128856),\n",
       "    np.float64(-192.34643934643628)]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(1.3141188843139466),\n",
       "   'mean_inference_ms': np.float64(5.375802308425255),\n",
       "   'mean_action_processing_ms': np.float64(0.6453345740574232),\n",
       "   'mean_env_wait_ms': np.float64(6.361957551955223),\n",
       "   'mean_env_render_ms': np.float64(0.0)},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.005188087622324626),\n",
       "   'StateBufferConnector_ms': np.float64(0.002494951089223226),\n",
       "   'ViewRequirementAgentConnector_ms': np.float64(0.06810774405797322)},\n",
       "  'num_episodes': 8,\n",
       "  'episode_return_max': np.float64(-327.1718818863369),\n",
       "  'episode_return_min': np.float64(-651.0568312071787),\n",
       "  'episode_return_mean': np.float64(-521.166285331625),\n",
       "  'episodes_this_iter': 8},\n",
       " 'num_healthy_workers': 2,\n",
       " 'num_in_flight_async_sample_reqs': 0,\n",
       " 'num_remote_worker_restarts': 0,\n",
       " 'num_agent_steps_sampled': 24000,\n",
       " 'num_agent_steps_trained': 24000,\n",
       " 'num_env_steps_sampled': 4000,\n",
       " 'num_env_steps_trained': 4000,\n",
       " 'num_env_steps_sampled_this_iter': 4000,\n",
       " 'num_env_steps_trained_this_iter': 4000,\n",
       " 'num_env_steps_sampled_throughput_per_sec': 13.793587026868678,\n",
       " 'num_env_steps_trained_throughput_per_sec': 13.793587026868678,\n",
       " 'timesteps_total': 4000,\n",
       " 'num_env_steps_sampled_lifetime': 4000,\n",
       " 'num_agent_steps_sampled_lifetime': 24000,\n",
       " 'num_steps_trained_this_iter': 4000,\n",
       " 'agent_timesteps_total': 24000,\n",
       " 'timers': {'training_iteration_time_ms': 289989.883,\n",
       "  'restore_workers_time_ms': 0.089,\n",
       "  'training_step_time_ms': 289989.692,\n",
       "  'sample_time_ms': 28168.457,\n",
       "  'learn_time_ms': 261805.355,\n",
       "  'learn_throughput': 15.279,\n",
       "  'synch_weights_time_ms': 15.047,\n",
       "  'restore_eval_workers_time_ms': 0.015,\n",
       "  'evaluation_iteration_time_ms': 70558.346,\n",
       "  'evaluation_iteration_throughput': 70.863},\n",
       " 'counters': {'num_env_steps_sampled_for_evaluation_this_iter': 5000,\n",
       "  'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 4000,\n",
       "  'num_agent_steps_sampled': 24000,\n",
       "  'num_agent_steps_trained': 24000},\n",
       " 'done': False,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'date': '2024-09-03_18-00-53',\n",
       " 'timestamp': 1725386453,\n",
       " 'time_this_iter_s': 360.55868124961853,\n",
       " 'time_total_s': 360.55868124961853,\n",
       " 'pid': 1345171,\n",
       " 'hostname': 'e-bgbfbjbn-sfks6-0',\n",
       " 'node_ip': '10.42.142.216',\n",
       " 'config': {'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_for_main_process': 1,\n",
       "  'eager_tracing': True,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'torch_compile_learner': False,\n",
       "  'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
       "  'torch_compile_learner_dynamo_backend': 'inductor',\n",
       "  'torch_compile_learner_dynamo_mode': None,\n",
       "  'torch_compile_worker': False,\n",
       "  'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
       "  'torch_compile_worker_dynamo_mode': None,\n",
       "  'enable_rl_module_and_learner': False,\n",
       "  'enable_env_runner_and_connector_v2': False,\n",
       "  'env': '6_agent_env',\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  '_is_atari': None,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'action_mask_key': 'action_mask',\n",
       "  'env_runner_cls': None,\n",
       "  'num_env_runners': 2,\n",
       "  'num_envs_per_env_runner': 1,\n",
       "  'num_cpus_per_env_runner': 1,\n",
       "  'num_gpus_per_env_runner': 0,\n",
       "  'custom_resources_per_env_runner': {},\n",
       "  'validate_env_runners_after_construction': True,\n",
       "  'sample_timeout_s': 60.0,\n",
       "  '_env_to_module_connector': None,\n",
       "  'add_default_connectors_to_env_to_module_pipeline': True,\n",
       "  '_module_to_env_connector': None,\n",
       "  'add_default_connectors_to_module_to_env_pipeline': True,\n",
       "  'episode_lookback_horizon': 1,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'compress_observations': False,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'update_worker_filter_stats': True,\n",
       "  'use_worker_filter_stats': True,\n",
       "  'enable_connectors': True,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'num_learners': 0,\n",
       "  'num_gpus_per_learner': 0,\n",
       "  'num_cpus_per_learner': 1,\n",
       "  'local_gpu_idx': 0,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'grad_clip': None,\n",
       "  'grad_clip_by': 'global_norm',\n",
       "  'train_batch_size': 4000,\n",
       "  'train_batch_size_per_learner': None,\n",
       "  'model': {'_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_weights_initializer': None,\n",
       "   'fcnet_weights_initializer_config': None,\n",
       "   'fcnet_bias_initializer': None,\n",
       "   'fcnet_bias_initializer_config': None,\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'conv_kernel_initializer': None,\n",
       "   'conv_kernel_initializer_config': None,\n",
       "   'conv_bias_initializer': None,\n",
       "   'conv_bias_initializer_config': None,\n",
       "   'conv_transpose_kernel_initializer': None,\n",
       "   'conv_transpose_kernel_initializer_config': None,\n",
       "   'conv_transpose_bias_initializer': None,\n",
       "   'conv_transpose_bias_initializer_config': None,\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'post_fcnet_weights_initializer': None,\n",
       "   'post_fcnet_weights_initializer_config': None,\n",
       "   'post_fcnet_bias_initializer': None,\n",
       "   'post_fcnet_bias_initializer_config': None,\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   'lstm_weights_initializer': None,\n",
       "   'lstm_weights_initializer_config': None,\n",
       "   'lstm_bias_initializer': None,\n",
       "   'lstm_bias_initializer_config': None,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'encoder_latent_dim': None,\n",
       "   'always_check_shapes': False,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1},\n",
       "  '_learner_connector': None,\n",
       "  'add_default_connectors_to_learner_pipeline': True,\n",
       "  'learner_config_dict': {},\n",
       "  'optimizer': {},\n",
       "  'max_requests_in_flight_per_sampler_worker': 2,\n",
       "  '_learner_class': None,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'algorithm_config_overrides_per_module': {},\n",
       "  '_per_module_overrides': {},\n",
       "  'count_steps_by': 'env_steps',\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_mapping_fn': <function __main__.<lambda>(aid, *args, **kwargs)>,\n",
       "  'policies_to_train': None,\n",
       "  'policy_states_are_swappable': False,\n",
       "  'observation_fn': None,\n",
       "  'input_read_method': 'read_parquet',\n",
       "  'input_read_method_kwargs': {},\n",
       "  'input_read_schema': {},\n",
       "  'map_batches_kwargs': {},\n",
       "  'iter_batches_kwargs': {},\n",
       "  'prelearner_class': None,\n",
       "  'prelearner_module_synch_period': 10,\n",
       "  'dataset_num_iters_per_learner': None,\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': 1,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 120.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_force_reset_envs_before_iteration': True,\n",
       "  'evaluation_config': None,\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_env_runners': 0,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 0,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  '_run_training_always_in_thread': False,\n",
       "  '_evaluation_parallel_to_training_wo_thread': False,\n",
       "  'ignore_env_runner_failures': False,\n",
       "  'recreate_failed_env_runners': False,\n",
       "  'max_num_env_runner_restarts': 1000,\n",
       "  'delay_between_env_runner_restarts_s': 60.0,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_env_runner_failures_tolerance': 100,\n",
       "  'env_runner_health_probe_timeout_s': 30,\n",
       "  'env_runner_restore_timeout_s': 1800,\n",
       "  '_model_config_dict': {},\n",
       "  '_rl_module_spec': MultiAgentRLModuleSpec(marl_module_class=<class 'ray.rllib.core.rl_module.marl_module.MultiAgentRLModule'>, inference_only=False, module_specs={'pursuer_1': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None), 'pursuer_2': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None), 'pursuer_4': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None), 'pursuer_3': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None), 'pursuer_0': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None), 'pursuer_5': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None)}, load_state_path=None, modules_to_load=None),\n",
       "  '_AlgorithmConfig__prior_exploration_config': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_initialize_loss_from_dummy_batch': False,\n",
       "  '_dont_auto_sync_env_runner_states': False,\n",
       "  'simple_optimizer': True,\n",
       "  'policy_map_cache': -1,\n",
       "  'worker_cls': -1,\n",
       "  'synchronize_filters': -1,\n",
       "  'enable_async_evaluation': -1,\n",
       "  'custom_async_evaluation_function': -1,\n",
       "  '_enable_rl_module_api': -1,\n",
       "  'auto_wrap_old_gym_envs': -1,\n",
       "  'disable_env_checking': -1,\n",
       "  'always_attach_evaluation_results': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  '_disable_execution_plan_api': -1,\n",
       "  'lr_schedule': None,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'use_kl_loss': True,\n",
       "  'kl_coeff': 0.2,\n",
       "  'kl_target': 0.01,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'mini_batch_size_per_learner': None,\n",
       "  'num_sgd_iter': 30,\n",
       "  'shuffle_sequences': True,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'vf_share_layers': -1,\n",
       "  'lambda': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'policies': {'pursuer_1': (None, None, None, None),\n",
       "   'pursuer_2': (None, None, None, None),\n",
       "   'pursuer_4': (None, None, None, None),\n",
       "   'pursuer_3': (None, None, None, None),\n",
       "   'pursuer_0': (None, None, None, None),\n",
       "   'pursuer_5': (None, None, None, None)},\n",
       "  'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch'},\n",
       " 'time_since_restore': 360.55868124961853,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': np.float64(38.932853717026376),\n",
       "  'ram_util_percent': np.float64(9.687649880095922)}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resto_algo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Callbacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 18:44:53,270\tWARNING algorithm_config.py:4258 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "/opt/conda/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:557: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-09-06 18:44:54,756\tWARNING services.py:2017 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 66924544 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-09-06 18:44:55,926\tINFO worker.py:1772 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2024-09-06 18:45:02,023\tWARNING algorithm_config.py:4258 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "2024-09-06 18:45:02,027\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from ray.rllib.core.rl_module.marl_module import MultiAgentRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv, ParallelPettingZooEnv\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "parser = add_rllib_example_script_args(\n",
    "    default_iters=200,\n",
    "    default_timesteps=1000000,\n",
    "    default_reward=300,\n",
    ")\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation import Episode, RolloutWorker\n",
    "from ray.rllib.policy import Policy\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(\n",
    "        self,\n",
    "        *,\n",
    "        episode: Episode,\n",
    "        env_index: int,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        episode.hist_data[\"mean_agent_return_hist\"] = []\n",
    "\n",
    "    def on_episode_end(\n",
    "        self,\n",
    "        *,\n",
    "        episode: Episode,\n",
    "        env_index: int,\n",
    "        #worker: RolloutWorker,\n",
    "        #base_env: BaseEnv,\n",
    "        #policies: Dict[str, Policy],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        print(f\"Total Reward: {episode.total_reward:>5}\")\n",
    "        episode.custom_metrics[\"mean_agent_return\"] = episode.total_reward / len(policies)\n",
    "        episode.hist_data[\"mean_agent_return_hist\"].append( episode.total_reward / len(policies) )\n",
    "    \n",
    "    def on_train_result(self, *, algorithm, result: dict, **kwargs):\n",
    "        result[\"num_agents\"] = len(policies)\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "\n",
    "num_agents = 2\n",
    "register_env(f\"{num_agents}_agent_env\", lambda _: ParallelPettingZooEnv(waterworld_v4.parallel_env(n_pursuers=num_agents)))\n",
    "policies = {f\"pursuer_{i}\" for i in range(num_agents)}\n",
    "\n",
    "config = (\n",
    "    get_trainable_cls(\"PPO\")\n",
    "    .get_default_config()\n",
    "    .environment(f\"{num_agents}_agent_env\")\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        # Exact 1:1 mapping from AgentID to ModuleID.\n",
    "        policy_mapping_fn=(lambda aid, *args, **kwargs: aid),\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=MultiAgentRLModuleSpec(\n",
    "            module_specs={p: SingleAgentRLModuleSpec() for p in policies},\n",
    "        ),\n",
    "    )\n",
    "    #.evaluation(\n",
    "    #    evaluation_interval=1,\n",
    "    #)\n",
    "    .callbacks(MyCallbacks)\n",
    ")\n",
    "\n",
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RolloutWorker pid=2004060)\u001b[0m Total Reward: -331.3670194328563\n",
      "\u001b[36m(RolloutWorker pid=2004061)\u001b[0m Total Reward: -288.67849504476527\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'custom_metrics': {},\n",
       " 'episode_media': {},\n",
       " 'info': {'learner': {'pursuer_0': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(2.320964),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(8.972203861673673),\n",
       "     'policy_loss': np.float64(-0.007902693240127216),\n",
       "     'vf_loss': np.float64(8.97887042115132),\n",
       "     'vf_explained_var': np.float64(2.0121286312739053e-05),\n",
       "     'kl': np.float64(0.006180924484601747),\n",
       "     'entropy': np.float64(2.805913825829824),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)},\n",
       "   'pursuer_1': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(1.347748),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(9.761141789952914),\n",
       "     'policy_loss': np.float64(-0.007552182588309127),\n",
       "     'vf_loss': np.float64(9.76750585436821),\n",
       "     'vf_explained_var': np.float64(8.406924704710643e-06),\n",
       "     'kl': np.float64(0.005940726396905423),\n",
       "     'entropy': np.float64(2.7642680456240973),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)}},\n",
       "  'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 4000,\n",
       "  'num_agent_steps_sampled': 8000,\n",
       "  'num_agent_steps_trained': 8000},\n",
       " 'env_runners': {'episode_reward_max': np.float64(-125.48602682582),\n",
       "  'episode_reward_min': np.float64(-331.3670194328563),\n",
       "  'episode_reward_mean': np.float64(-265.8566420380654),\n",
       "  'episode_len_mean': np.float64(500.0),\n",
       "  'episode_media': {},\n",
       "  'episodes_timesteps_total': 4000,\n",
       "  'policy_reward_min': {'pursuer_0': np.float64(-111.74081459897249),\n",
       "   'pursuer_1': np.float64(-220.06249260425108)},\n",
       "  'policy_reward_max': {'pursuer_0': np.float64(-10.169353142187802),\n",
       "   'pursuer_1': np.float64(-115.3166736836321)},\n",
       "  'policy_reward_mean': {'pursuer_0': np.float64(-80.07442243230909),\n",
       "   'pursuer_1': np.float64(-185.78221960575627)},\n",
       "  'custom_metrics': {'mean_agent_return_mean': np.float64(-132.9283210190327),\n",
       "   'mean_agent_return_min': np.float64(-165.68350971642815),\n",
       "   'mean_agent_return_max': np.float64(-62.74301341291)},\n",
       "  'hist_stats': {'mean_agent_return_hist': [np.float64(-165.68350971642815),\n",
       "    np.float64(-110.91285790517045),\n",
       "    np.float64(-62.74301341291),\n",
       "    np.float64(-132.67366442417025),\n",
       "    np.float64(-155.59712155517653),\n",
       "    np.float64(-157.01245949858077),\n",
       "    np.float64(-134.46469411744275),\n",
       "    np.float64(-144.33924752238264)],\n",
       "   'episode_reward': [np.float64(-331.3670194328563),\n",
       "    np.float64(-221.8257158103409),\n",
       "    np.float64(-125.48602682582),\n",
       "    np.float64(-265.3473288483405),\n",
       "    np.float64(-311.19424311035306),\n",
       "    np.float64(-314.02491899716154),\n",
       "    np.float64(-268.9293882348855),\n",
       "    np.float64(-288.67849504476527)],\n",
       "   'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500],\n",
       "   'policy_pursuer_0_reward': [np.float64(-111.48104344743778),\n",
       "    np.float64(-66.25442285780345),\n",
       "    np.float64(-10.169353142187802),\n",
       "    np.float64(-67.21351067801459),\n",
       "    np.float64(-111.74081459897249),\n",
       "    np.float64(-93.96242639291076),\n",
       "    np.float64(-92.53150479262305),\n",
       "    np.float64(-87.24230354852273)],\n",
       "   'policy_pursuer_1_reward': [np.float64(-219.88597598541836),\n",
       "    np.float64(-155.57129295253708),\n",
       "    np.float64(-115.3166736836321),\n",
       "    np.float64(-198.1338181703259),\n",
       "    np.float64(-199.45342851138057),\n",
       "    np.float64(-220.06249260425108),\n",
       "    np.float64(-176.39788344226238),\n",
       "    np.float64(-201.43619149624283)]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(0.4040603337438031),\n",
       "   'mean_inference_ms': np.float64(1.3968703867136867),\n",
       "   'mean_action_processing_ms': np.float64(0.1971256369534044),\n",
       "   'mean_env_wait_ms': np.float64(1.7226933837234824),\n",
       "   'mean_env_render_ms': np.float64(0.0)},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.00502169132232666),\n",
       "   'StateBufferConnector_ms': np.float64(0.0025987625122070312),\n",
       "   'ViewRequirementAgentConnector_ms': np.float64(0.06003826856613159)},\n",
       "  'num_episodes': 8,\n",
       "  'episode_return_max': np.float64(-125.48602682582),\n",
       "  'episode_return_min': np.float64(-331.3670194328563),\n",
       "  'episode_return_mean': np.float64(-265.8566420380654),\n",
       "  'episodes_this_iter': 8},\n",
       " 'num_healthy_workers': 2,\n",
       " 'num_in_flight_async_sample_reqs': 0,\n",
       " 'num_remote_worker_restarts': 0,\n",
       " 'num_agent_steps_sampled': 8000,\n",
       " 'num_agent_steps_trained': 8000,\n",
       " 'num_env_steps_sampled': 4000,\n",
       " 'num_env_steps_trained': 4000,\n",
       " 'num_env_steps_sampled_this_iter': 4000,\n",
       " 'num_env_steps_trained_this_iter': 4000,\n",
       " 'num_env_steps_sampled_throughput_per_sec': 42.54940475932031,\n",
       " 'num_env_steps_trained_throughput_per_sec': 42.54940475932031,\n",
       " 'timesteps_total': 4000,\n",
       " 'num_env_steps_sampled_lifetime': 4000,\n",
       " 'num_agent_steps_sampled_lifetime': 8000,\n",
       " 'num_steps_trained_this_iter': 4000,\n",
       " 'agent_timesteps_total': 8000,\n",
       " 'timers': {'training_iteration_time_ms': 94008.426,\n",
       "  'restore_workers_time_ms': 0.042,\n",
       "  'training_step_time_ms': 94008.277,\n",
       "  'sample_time_ms': 7515.277,\n",
       "  'learn_time_ms': 86471.969,\n",
       "  'learn_throughput': 46.258,\n",
       "  'synch_weights_time_ms': 20.405},\n",
       " 'counters': {'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 4000,\n",
       "  'num_agent_steps_sampled': 8000,\n",
       "  'num_agent_steps_trained': 8000},\n",
       " 'done': False,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'date': '2024-09-06_18-46-38',\n",
       " 'timestamp': 1725648398,\n",
       " 'time_this_iter_s': 94.02253270149231,\n",
       " 'time_total_s': 94.02253270149231,\n",
       " 'pid': 1860436,\n",
       " 'hostname': 'e-bgbfbjbn-sfks6-0',\n",
       " 'node_ip': '10.42.142.216',\n",
       " 'config': {'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_for_main_process': 1,\n",
       "  'eager_tracing': True,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'torch_compile_learner': False,\n",
       "  'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
       "  'torch_compile_learner_dynamo_backend': 'inductor',\n",
       "  'torch_compile_learner_dynamo_mode': None,\n",
       "  'torch_compile_worker': False,\n",
       "  'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
       "  'torch_compile_worker_dynamo_mode': None,\n",
       "  'enable_rl_module_and_learner': False,\n",
       "  'enable_env_runner_and_connector_v2': False,\n",
       "  'env': '2_agent_env',\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  '_is_atari': None,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'action_mask_key': 'action_mask',\n",
       "  'env_runner_cls': None,\n",
       "  'num_env_runners': 2,\n",
       "  'num_envs_per_env_runner': 1,\n",
       "  'num_cpus_per_env_runner': 1,\n",
       "  'num_gpus_per_env_runner': 0,\n",
       "  'custom_resources_per_env_runner': {},\n",
       "  'validate_env_runners_after_construction': True,\n",
       "  'sample_timeout_s': 60.0,\n",
       "  '_env_to_module_connector': None,\n",
       "  'add_default_connectors_to_env_to_module_pipeline': True,\n",
       "  '_module_to_env_connector': None,\n",
       "  'add_default_connectors_to_module_to_env_pipeline': True,\n",
       "  'episode_lookback_horizon': 1,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'compress_observations': False,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'update_worker_filter_stats': True,\n",
       "  'use_worker_filter_stats': True,\n",
       "  'enable_connectors': True,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'num_learners': 0,\n",
       "  'num_gpus_per_learner': 0,\n",
       "  'num_cpus_per_learner': 1,\n",
       "  'local_gpu_idx': 0,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'grad_clip': None,\n",
       "  'grad_clip_by': 'global_norm',\n",
       "  'train_batch_size': 4000,\n",
       "  'train_batch_size_per_learner': None,\n",
       "  'model': {'_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_weights_initializer': None,\n",
       "   'fcnet_weights_initializer_config': None,\n",
       "   'fcnet_bias_initializer': None,\n",
       "   'fcnet_bias_initializer_config': None,\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'conv_kernel_initializer': None,\n",
       "   'conv_kernel_initializer_config': None,\n",
       "   'conv_bias_initializer': None,\n",
       "   'conv_bias_initializer_config': None,\n",
       "   'conv_transpose_kernel_initializer': None,\n",
       "   'conv_transpose_kernel_initializer_config': None,\n",
       "   'conv_transpose_bias_initializer': None,\n",
       "   'conv_transpose_bias_initializer_config': None,\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'post_fcnet_weights_initializer': None,\n",
       "   'post_fcnet_weights_initializer_config': None,\n",
       "   'post_fcnet_bias_initializer': None,\n",
       "   'post_fcnet_bias_initializer_config': None,\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   'lstm_weights_initializer': None,\n",
       "   'lstm_weights_initializer_config': None,\n",
       "   'lstm_bias_initializer': None,\n",
       "   'lstm_bias_initializer_config': None,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'encoder_latent_dim': None,\n",
       "   'always_check_shapes': False,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1},\n",
       "  '_learner_connector': None,\n",
       "  'add_default_connectors_to_learner_pipeline': True,\n",
       "  'learner_config_dict': {},\n",
       "  'optimizer': {},\n",
       "  'max_requests_in_flight_per_sampler_worker': 2,\n",
       "  '_learner_class': None,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'algorithm_config_overrides_per_module': {},\n",
       "  '_per_module_overrides': {},\n",
       "  'count_steps_by': 'env_steps',\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_mapping_fn': <function __main__.<lambda>(aid, *args, **kwargs)>,\n",
       "  'policies_to_train': None,\n",
       "  'policy_states_are_swappable': False,\n",
       "  'observation_fn': None,\n",
       "  'input_read_method': 'read_parquet',\n",
       "  'input_read_method_kwargs': {},\n",
       "  'input_read_schema': {},\n",
       "  'map_batches_kwargs': {},\n",
       "  'iter_batches_kwargs': {},\n",
       "  'prelearner_class': None,\n",
       "  'prelearner_module_synch_period': 10,\n",
       "  'dataset_num_iters_per_learner': None,\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 120.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_force_reset_envs_before_iteration': True,\n",
       "  'evaluation_config': None,\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_env_runners': 0,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 0,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  '_run_training_always_in_thread': False,\n",
       "  '_evaluation_parallel_to_training_wo_thread': False,\n",
       "  'ignore_env_runner_failures': False,\n",
       "  'recreate_failed_env_runners': False,\n",
       "  'max_num_env_runner_restarts': 1000,\n",
       "  'delay_between_env_runner_restarts_s': 60.0,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_env_runner_failures_tolerance': 100,\n",
       "  'env_runner_health_probe_timeout_s': 30,\n",
       "  'env_runner_restore_timeout_s': 1800,\n",
       "  '_model_config_dict': {},\n",
       "  '_rl_module_spec': MultiAgentRLModuleSpec(marl_module_class=<class 'ray.rllib.core.rl_module.marl_module.MultiAgentRLModule'>, inference_only=False, module_specs={'pursuer_0': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None), 'pursuer_1': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None)}, load_state_path=None, modules_to_load=None),\n",
       "  '_AlgorithmConfig__prior_exploration_config': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_initialize_loss_from_dummy_batch': False,\n",
       "  '_dont_auto_sync_env_runner_states': False,\n",
       "  'simple_optimizer': True,\n",
       "  'policy_map_cache': -1,\n",
       "  'worker_cls': -1,\n",
       "  'synchronize_filters': -1,\n",
       "  'enable_async_evaluation': -1,\n",
       "  'custom_async_evaluation_function': -1,\n",
       "  '_enable_rl_module_api': -1,\n",
       "  'auto_wrap_old_gym_envs': -1,\n",
       "  'disable_env_checking': -1,\n",
       "  'always_attach_evaluation_results': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  '_disable_execution_plan_api': -1,\n",
       "  'lr_schedule': None,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'use_kl_loss': True,\n",
       "  'kl_coeff': 0.2,\n",
       "  'kl_target': 0.01,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'mini_batch_size_per_learner': None,\n",
       "  'num_sgd_iter': 30,\n",
       "  'shuffle_sequences': True,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'vf_share_layers': -1,\n",
       "  'lambda': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'policies': {'pursuer_0': (None, None, None, None),\n",
       "   'pursuer_1': (None, None, None, None)},\n",
       "  'callbacks': __main__.MyCallbacks,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch'},\n",
       " 'time_since_restore': 94.02253270149231,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': np.float64(49.28840579710145),\n",
       "  'ram_util_percent': np.float64(9.731884057971016)},\n",
       " 'num_agents': 2}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
