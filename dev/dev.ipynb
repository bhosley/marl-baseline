{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ray[rllib,tune]\n",
    "#!pip install pettingzoo pygame pymunk\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-06 19:59:19,890\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-09-06 19:59:22,078\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.core.rl_module.marl_module import MultiAgentRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "\n",
    "from pettingzoo.sisl import waterworld_v4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 02:06:20,467\tWARNING algorithm_config.py:4258 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "/opt/conda/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:557: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-08-29 02:06:21,596\tWARNING services.py:2017 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67084288 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-08-29 02:06:21,682\tINFO worker.py:1772 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2024-08-29 02:06:26,362\tWARNING algorithm_config.py:4258 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "2024-08-29 02:06:26,405\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from glob import glob\n",
    "from os import path\n",
    "from ray.rllib.policy.policy import Policy\n",
    "\n",
    "ray.shutdown() \n",
    "\n",
    "# Trained to about 0 combined return\n",
    "checkpoint_path = \"/root/ray_results/PPO_2024-08-28_20-57-45/PPO_2_agent_env_2cf59_00000_0_2024-08-28_20-57-45/checkpoint_000000\"\n",
    "pols = glob(checkpoint_path+\"/policies/*\")\n",
    "specs = {path.basename(p) : Policy.from_checkpoint(p) for p in pols}\n",
    "#specs = {path.basename(p) : SingleAgentRLModuleSpec(load_state_path=p) for p in pols} # Non-deterministic policy weight return (implies new)\n",
    "\n",
    "\n",
    "num_agents = 2\n",
    "\n",
    "register_env(f\"{num_agents}_agent_env\", lambda _: ParallelPettingZooEnv(waterworld_v4.parallel_env(n_pursuers=num_agents)))\n",
    "policies = {f\"pursuer_{i}\" for i in range(num_agents)}\n",
    "\n",
    "\n",
    "resto_config = (\n",
    "    get_trainable_cls(\"PPO\")\n",
    "    .get_default_config()\n",
    "    .environment(f\"{num_agents}_agent_env\")\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        # Exact 1:1 mapping from AgentID to ModuleID.\n",
    "        policy_mapping_fn=(lambda aid, *args, **kwargs: aid),\n",
    "    )\n",
    "    .rl_module(\n",
    "        #model_config_dict={\"vf_share_layers\": True},\n",
    "        rl_module_spec=MultiAgentRLModuleSpec(\n",
    "            #load_state_path=\n",
    "            #module_specs=specs,\n",
    "            module_specs={p: SingleAgentRLModuleSpec() for p in policies},\n",
    "        ),\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "resto_algo = resto_config.build()\n",
    "resto_algo.get_policy(\"pursuer_0\").set_weights(specs[\"pursuer_0\"].get_weights())\n",
    "resto_algo.get_policy(\"pursuer_1\").set_weights(specs[\"pursuer_1\"].get_weights())\n",
    "#resto_algo.get_policy(\"pursuer_0\").get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we restore an algo to its original size, and evaluate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env_runners': {'episode_reward_max': np.float64(104.48447046235398),\n",
       "  'episode_reward_min': np.float64(-49.97758701259906),\n",
       "  'episode_reward_mean': np.float64(34.31495169008534),\n",
       "  'episode_len_mean': np.float64(500.0),\n",
       "  'episode_media': {},\n",
       "  'episodes_timesteps_total': 5000,\n",
       "  'policy_reward_min': {'pursuer_0': np.float64(-27.80399612520721),\n",
       "   'pursuer_1': np.float64(-74.73581968384396)},\n",
       "  'policy_reward_max': {'pursuer_0': np.float64(108.50397211584374),\n",
       "   'pursuer_1': np.float64(66.59594375510522)},\n",
       "  'policy_reward_mean': {'pursuer_0': np.float64(32.97170784631929),\n",
       "   'pursuer_1': np.float64(1.3432438437660394)},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [np.float64(44.487376281462424),\n",
       "    np.float64(98.65380257932057),\n",
       "    np.float64(84.50964155764572),\n",
       "    np.float64(20.08502968193733),\n",
       "    np.float64(104.48447046235398),\n",
       "    np.float64(-38.577462399422544),\n",
       "    np.float64(-49.97758701259906),\n",
       "    np.float64(20.465628189378837),\n",
       "    np.float64(91.2633147856224),\n",
       "    np.float64(-32.24469722484628)],\n",
       "   'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500],\n",
       "   'policy_pursuer_0_reward': [np.float64(40.07564281289912),\n",
       "    np.float64(53.608308921809055),\n",
       "    np.float64(70.12441925264447),\n",
       "    np.float64(-27.80399612520721),\n",
       "    np.float64(37.88852670724881),\n",
       "    np.float64(-4.464299391091167),\n",
       "    np.float64(4.619717553032),\n",
       "    np.float64(4.673664157016454),\n",
       "    np.float64(108.50397211584374),\n",
       "    np.float64(42.49112245899767)],\n",
       "   'policy_pursuer_1_reward': [np.float64(4.411733468563258),\n",
       "    np.float64(45.04549365751143),\n",
       "    np.float64(14.38522230500113),\n",
       "    np.float64(47.889025807144655),\n",
       "    np.float64(66.59594375510522),\n",
       "    np.float64(-34.113163008331284),\n",
       "    np.float64(-54.59730456563125),\n",
       "    np.float64(15.79196403236251),\n",
       "    np.float64(-17.24065733022132),\n",
       "    np.float64(-74.73581968384396)]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(0.337330916680976),\n",
       "   'mean_inference_ms': np.float64(1.317882094427581),\n",
       "   'mean_action_processing_ms': np.float64(0.18205562599562794),\n",
       "   'mean_env_wait_ms': np.float64(1.6657234775866285),\n",
       "   'mean_env_render_ms': np.float64(0.0)},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.0024259090423583984),\n",
       "   'StateBufferConnector_ms': np.float64(0.0023412704467773438),\n",
       "   'ViewRequirementAgentConnector_ms': np.float64(0.05772590637207031)},\n",
       "  'num_episodes': 10,\n",
       "  'episode_return_max': np.float64(104.48447046235398),\n",
       "  'episode_return_min': np.float64(-49.97758701259906),\n",
       "  'episode_return_mean': np.float64(34.31495169008534),\n",
       "  'episodes_this_iter': 10},\n",
       " 'num_agent_steps_sampled_this_iter': 10000,\n",
       " 'num_env_steps_sampled_this_iter': 5000,\n",
       " 'timesteps_this_iter': 5000}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resto_algo.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets try with a different number of test agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/opt/conda/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:164: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2024-09-03 17:50:57,409\tWARNING algorithm_config.py:4258 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "/opt/conda/lib/python3.11/site-packages/ray/rllib/algorithms/algorithm.py:557: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/conda/lib/python3.11/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-09-03 17:50:58,981\tWARNING services.py:2017 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67059712 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-09-03 17:51:00,086\tINFO worker.py:1772 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "2024-09-03 17:51:06,518\tWARNING algorithm_config.py:4258 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "2024-09-03 17:51:06,899\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'env_runners': {'episode_reward_max': np.float64(-133.08651258384768),\n",
       "  'episode_reward_min': np.float64(-429.6043485772751),\n",
       "  'episode_reward_mean': np.float64(-279.01460767396213),\n",
       "  'episode_len_mean': np.float64(500.0),\n",
       "  'episode_media': {},\n",
       "  'episodes_timesteps_total': 5000,\n",
       "  'policy_reward_min': {'pursuer_0': np.float64(-70.69341337433875),\n",
       "   'pursuer_1': np.float64(-83.28156152153042),\n",
       "   'pursuer_2': np.float64(-89.37824978401237),\n",
       "   'pursuer_3': np.float64(-64.26033845731828),\n",
       "   'pursuer_4': np.float64(-104.50749893251731),\n",
       "   'pursuer_5': np.float64(-180.21766134359572)},\n",
       "  'policy_reward_max': {'pursuer_0': np.float64(-8.571273725909617),\n",
       "   'pursuer_1': np.float64(60.30311215169101),\n",
       "   'pursuer_2': np.float64(42.74708175708947),\n",
       "   'pursuer_3': np.float64(-7.6108996017414645),\n",
       "   'pursuer_4': np.float64(67.86120096904311),\n",
       "   'pursuer_5': np.float64(-44.10802049107301)},\n",
       "  'policy_reward_mean': {'pursuer_0': np.float64(-42.284014583047856),\n",
       "   'pursuer_1': np.float64(-40.1377328008583),\n",
       "   'pursuer_2': np.float64(-40.48262924096856),\n",
       "   'pursuer_3': np.float64(-45.66979861051086),\n",
       "   'pursuer_4': np.float64(1.1955199344886225),\n",
       "   'pursuer_5': np.float64(-111.63595237306478)},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [np.float64(-166.19797254025622),\n",
       "    np.float64(-326.0186364467571),\n",
       "    np.float64(-345.568169571401),\n",
       "    np.float64(-429.6043485772751),\n",
       "    np.float64(-220.67587333793387),\n",
       "    np.float64(-185.92611986327765),\n",
       "    np.float64(-262.3994365645389),\n",
       "    np.float64(-415.2977044772379),\n",
       "    np.float64(-133.08651258384768),\n",
       "    np.float64(-305.3713027770962)],\n",
       "   'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500],\n",
       "   'policy_pursuer_0_reward': [np.float64(-19.490907228469617),\n",
       "    np.float64(-41.93875258924693),\n",
       "    np.float64(-8.571273725909617),\n",
       "    np.float64(-70.69341337433875),\n",
       "    np.float64(-36.23102767416677),\n",
       "    np.float64(-53.02094782426345),\n",
       "    np.float64(-40.14230482430377),\n",
       "    np.float64(-53.63067677345778),\n",
       "    np.float64(-58.96140226997416),\n",
       "    np.float64(-40.1594395463477)],\n",
       "   'policy_pursuer_1_reward': [np.float64(-20.32164907733235),\n",
       "    np.float64(-63.36892477087198),\n",
       "    np.float64(-78.8616327184739),\n",
       "    np.float64(-83.28156152153042),\n",
       "    np.float64(-17.192987182804654),\n",
       "    np.float64(60.30311215169101),\n",
       "    np.float64(-81.58622316165038),\n",
       "    np.float64(15.66510774069288),\n",
       "    np.float64(-59.336612818969634),\n",
       "    np.float64(-73.39595664933358)],\n",
       "   'policy_pursuer_2_reward': [np.float64(42.74708175708947),\n",
       "    np.float64(-48.91667205879994),\n",
       "    np.float64(-60.285853924302046),\n",
       "    np.float64(-75.61061275050122),\n",
       "    np.float64(-50.681475798239205),\n",
       "    np.float64(-28.212178049285676),\n",
       "    np.float64(-24.130360475552504),\n",
       "    np.float64(-89.37824978401237),\n",
       "    np.float64(-13.983339211166555),\n",
       "    np.float64(-56.37463211491561)],\n",
       "   'policy_pursuer_3_reward': [np.float64(-56.420543595035035),\n",
       "    np.float64(-30.623046206764013),\n",
       "    np.float64(-57.08194477161877),\n",
       "    np.float64(-51.94511833327789),\n",
       "    np.float64(-33.07761992343081),\n",
       "    np.float64(-64.26033845731828),\n",
       "    np.float64(-35.91195257054746),\n",
       "    np.float64(-61.13473613175747),\n",
       "    np.float64(-7.6108996017414645),\n",
       "    np.float64(-58.631786513617456)],\n",
       "   'policy_pursuer_4_reward': [np.float64(-33.06575598603719),\n",
       "    np.float64(-3.5606353837811464),\n",
       "    np.float64(39.45019691249885),\n",
       "    np.float64(29.81497359985701),\n",
       "    np.float64(-9.630923310699172),\n",
       "    np.float64(9.37285418175031),\n",
       "    np.float64(48.922254756580564),\n",
       "    np.float64(-104.50749893251731),\n",
       "    np.float64(67.86120096904311),\n",
       "    np.float64(-32.70146746180881)],\n",
       "   'policy_pursuer_5_reward': [np.float64(-79.64619841047161),\n",
       "    np.float64(-137.61060543729332),\n",
       "    np.float64(-180.21766134359572),\n",
       "    np.float64(-177.88861619748133),\n",
       "    np.float64(-73.86183944859376),\n",
       "    np.float64(-110.1086218658516),\n",
       "    np.float64(-129.55085028906458),\n",
       "    np.float64(-122.31165059618439),\n",
       "    np.float64(-61.05545965103868),\n",
       "    np.float64(-44.10802049107301)]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(1.183615472645217),\n",
       "   'mean_inference_ms': np.float64(5.674096303709839),\n",
       "   'mean_action_processing_ms': np.float64(0.6120333169083957),\n",
       "   'mean_env_wait_ms': np.float64(6.530168032174205),\n",
       "   'mean_env_render_ms': np.float64(0.0)},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.005753835042317708),\n",
       "   'StateBufferConnector_ms': np.float64(0.002828836441040039),\n",
       "   'ViewRequirementAgentConnector_ms': np.float64(0.07375001907348633)},\n",
       "  'num_episodes': 10,\n",
       "  'episode_return_max': np.float64(-133.08651258384768),\n",
       "  'episode_return_min': np.float64(-429.6043485772751),\n",
       "  'episode_return_mean': np.float64(-279.01460767396213),\n",
       "  'episodes_this_iter': 10},\n",
       " 'num_agent_steps_sampled_this_iter': 30000,\n",
       " 'num_env_steps_sampled_this_iter': 5000,\n",
       " 'timesteps_this_iter': 5000}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from glob import glob\n",
    "from os import path\n",
    "from ray.rllib.policy.policy import Policy\n",
    "import numpy as np\n",
    "\n",
    "ray.shutdown() \n",
    "\n",
    "# Trained to about 0 combined return\n",
    "checkpoint_path = \"/root/ray_results/PPO_2024-08-28_20-57-45/PPO_2_agent_env_2cf59_00000_0_2024-08-28_20-57-45/checkpoint_000000\"\n",
    "pols = glob(checkpoint_path+\"/policies/*\")\n",
    "specs = {path.basename(p) : Policy.from_checkpoint(p) for p in pols}\n",
    "#specs = {path.basename(p) : SingleAgentRLModuleSpec(load_state_path=p) for p in pols} # Non-deterministic policy weight return (implies new)\n",
    "\n",
    "num_trained_agents = 2\n",
    "num_test_agents = 6\n",
    "\n",
    "register_env(f\"{num_test_agents}_agent_env\", lambda _: ParallelPettingZooEnv(waterworld_v4.parallel_env(n_pursuers=num_test_agents)))\n",
    "policies = {f\"pursuer_{i}\" for i in range(num_test_agents)}\n",
    "\n",
    "\n",
    "resto_config = (\n",
    "    get_trainable_cls(\"PPO\")\n",
    "    .get_default_config()\n",
    "    .environment(f\"{num_test_agents}_agent_env\")\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        # Exact 1:1 mapping from AgentID to ModuleID.\n",
    "        policy_mapping_fn=(lambda aid, *args, **kwargs: aid),\n",
    "    )\n",
    "    .rl_module(\n",
    "        #model_config_dict={\"vf_share_layers\": True},\n",
    "        rl_module_spec=MultiAgentRLModuleSpec(\n",
    "            #load_state_path=\n",
    "            #module_specs=specs,\n",
    "            module_specs={p: SingleAgentRLModuleSpec() for p in policies},\n",
    "        ),\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_interval=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "resto_algo = resto_config.build()\n",
    "for test_id in range(num_test_agents):\n",
    "    train_id = np.random.randint(num_trained_agents)\n",
    "    resto_algo.get_policy(f\"pursuer_{test_id}\").set_weights(specs[f\"pursuer_{train_id}\"].get_weights())\n",
    "\n",
    "#resto_algo.get_policy(\"pursuer_0\").set_weights(specs[\"pursuer_0\"].get_weights())\n",
    "#resto_algo.get_policy(\"pursuer_1\").set_weights(specs[\"pursuer_1\"].get_weights())\n",
    "#resto_algo.get_policy(\"pursuer_0\").get_weights()\n",
    "\n",
    "resto_algo.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-03 17:55:20,672\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'env_runners': {'episode_reward_max': np.float64(-364.769848341295),\n",
       "   'episode_reward_min': np.float64(-619.0375683355406),\n",
       "   'episode_reward_mean': np.float64(-493.78093420107905),\n",
       "   'episode_len_mean': np.float64(500.0),\n",
       "   'episode_media': {},\n",
       "   'episodes_timesteps_total': 5000,\n",
       "   'policy_reward_min': {'pursuer_0': np.float64(-113.90485384019159),\n",
       "    'pursuer_1': np.float64(-109.27251741852379),\n",
       "    'pursuer_2': np.float64(-114.55402342215696),\n",
       "    'pursuer_3': np.float64(-111.76090622319133),\n",
       "    'pursuer_4': np.float64(-109.0597747992956),\n",
       "    'pursuer_5': np.float64(-217.50166684483233)},\n",
       "   'policy_reward_max': {'pursuer_0': np.float64(11.303415129637894),\n",
       "    'pursuer_1': np.float64(0.2069972847033158),\n",
       "    'pursuer_2': np.float64(-46.84567547040106),\n",
       "    'pursuer_3': np.float64(-6.933874989835202),\n",
       "    'pursuer_4': np.float64(-7.903867561959164),\n",
       "    'pursuer_5': np.float64(-102.74657319368887)},\n",
       "   'policy_reward_mean': {'pursuer_0': np.float64(-62.68889913535977),\n",
       "    'pursuer_1': np.float64(-65.25422536988678),\n",
       "    'pursuer_2': np.float64(-74.27730941244923),\n",
       "    'pursuer_3': np.float64(-65.3008027191293),\n",
       "    'pursuer_4': np.float64(-63.75138013879174),\n",
       "    'pursuer_5': np.float64(-162.50831742546185)},\n",
       "   'custom_metrics': {},\n",
       "   'hist_stats': {'episode_reward': [np.float64(-512.3823702423662),\n",
       "     np.float64(-508.5460752882049),\n",
       "     np.float64(-613.3155297566499),\n",
       "     np.float64(-364.769848341295),\n",
       "     np.float64(-495.4410446420681),\n",
       "     np.float64(-619.0375683355406),\n",
       "     np.float64(-389.5228384962124),\n",
       "     np.float64(-382.61291184917025),\n",
       "     np.float64(-603.3023013627752),\n",
       "     np.float64(-448.8788536965084)],\n",
       "    'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500, 500, 500],\n",
       "    'policy_pursuer_0_reward': [np.float64(-60.37390249750134),\n",
       "     np.float64(-60.741754372675686),\n",
       "     np.float64(-92.38409091020394),\n",
       "     np.float64(11.303415129637894),\n",
       "     np.float64(-89.95122930874328),\n",
       "     np.float64(-113.90485384019159),\n",
       "     np.float64(-33.69541775768187),\n",
       "     np.float64(-57.605645430906925),\n",
       "     np.float64(-82.23981599404104),\n",
       "     np.float64(-47.29569637128995)],\n",
       "    'policy_pursuer_1_reward': [np.float64(-69.16587427966073),\n",
       "     np.float64(-49.64229550423252),\n",
       "     np.float64(-96.41777564875132),\n",
       "     np.float64(-69.87433716798475),\n",
       "     np.float64(-109.27251741852379),\n",
       "     np.float64(-89.99336501878994),\n",
       "     np.float64(0.2069972847033158),\n",
       "     np.float64(-10.887723201806109),\n",
       "     np.float64(-93.20806475836568),\n",
       "     np.float64(-64.28729798545638)],\n",
       "    'policy_pursuer_2_reward': [np.float64(-89.8094922272591),\n",
       "     np.float64(-68.80568767053518),\n",
       "     np.float64(-114.55402342215696),\n",
       "     np.float64(-70.71135157105256),\n",
       "     np.float64(-75.0275389938279),\n",
       "     np.float64(-47.789534609511726),\n",
       "     np.float64(-46.88852374743173),\n",
       "     np.float64(-91.35057806874335),\n",
       "     np.float64(-46.84567547040106),\n",
       "     np.float64(-90.99068834357274)],\n",
       "    'policy_pursuer_3_reward': [np.float64(-111.32630440205934),\n",
       "     np.float64(-103.95080333396903),\n",
       "     np.float64(-84.83800127252131),\n",
       "     np.float64(-6.933874989835202),\n",
       "     np.float64(-8.589331807829799),\n",
       "     np.float64(-111.76090622319133),\n",
       "     np.float64(-30.30373001435348),\n",
       "     np.float64(-11.583014317344874),\n",
       "     np.float64(-110.78242937291465),\n",
       "     np.float64(-72.93963145727405)],\n",
       "    'policy_pursuer_4_reward': [np.float64(-66.7274235142958),\n",
       "     np.float64(-7.903867561959164),\n",
       "     np.float64(-54.9051831086639),\n",
       "     np.float64(-109.0597747992956),\n",
       "     np.float64(-52.28148786129459),\n",
       "     np.float64(-71.4751414424136),\n",
       "     np.float64(-67.6893324307532),\n",
       "     np.float64(-46.52229002548692),\n",
       "     np.float64(-90.3303342985284),\n",
       "     np.float64(-70.61896634522623)],\n",
       "    'policy_pursuer_5_reward': [np.float64(-114.97937332158938),\n",
       "     np.float64(-217.50166684483233),\n",
       "     np.float64(-170.21645539435244),\n",
       "     np.float64(-119.49392494276402),\n",
       "     np.float64(-160.31893925184878),\n",
       "     np.float64(-184.113767201442),\n",
       "     np.float64(-211.15283183069522),\n",
       "     np.float64(-164.66366080488214),\n",
       "     np.float64(-179.89598146852364),\n",
       "     np.float64(-102.74657319368887)]},\n",
       "   'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(1.2004366875553045),\n",
       "    'mean_inference_ms': np.float64(5.637246124172983),\n",
       "    'mean_action_processing_ms': np.float64(0.6170369138623248),\n",
       "    'mean_env_wait_ms': np.float64(6.577228381745566),\n",
       "    'mean_env_render_ms': np.float64(0.0)},\n",
       "   'num_faulty_episodes': 0,\n",
       "   'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.0030291080474853516),\n",
       "    'StateBufferConnector_ms': np.float64(0.002672274907430013),\n",
       "    'ViewRequirementAgentConnector_ms': np.float64(0.0721895694732666)},\n",
       "   'num_episodes': 10,\n",
       "   'episode_return_max': np.float64(-364.769848341295),\n",
       "   'episode_return_min': np.float64(-619.0375683355406),\n",
       "   'episode_return_mean': np.float64(-493.78093420107905),\n",
       "   'episodes_this_iter': 10},\n",
       "  'num_agent_steps_sampled_this_iter': 30000,\n",
       "  'num_env_steps_sampled_this_iter': 5000,\n",
       "  'timesteps_this_iter': 5000,\n",
       "  'num_healthy_workers': 0,\n",
       "  'num_in_flight_async_reqs': 0,\n",
       "  'num_remote_worker_restarts': 0},\n",
       " 'custom_metrics': {},\n",
       " 'episode_media': {},\n",
       " 'info': {'learner': {'pursuer_1': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(8.284326),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(6.566026151676973),\n",
       "     'policy_loss': np.float64(0.09367009556881385),\n",
       "     'vf_loss': np.float64(6.283298968772093),\n",
       "     'vf_explained_var': np.float64(0.08557038928071657),\n",
       "     'kl': np.float64(0.9452853819413576),\n",
       "     'entropy': np.float64(3.36656474818786),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)},\n",
       "   'pursuer_2': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(9.113566),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(6.077735746900241),\n",
       "     'policy_loss': np.float64(0.0958497002410392),\n",
       "     'vf_loss': np.float64(5.798774990439415),\n",
       "     'vf_explained_var': np.float64(0.037562563084065914),\n",
       "     'kl': np.float64(0.9155552069811771),\n",
       "     'entropy': np.float64(3.340836751833558),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)},\n",
       "   'pursuer_4': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(5.2964835),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(6.91358299801747),\n",
       "     'policy_loss': np.float64(0.06978534823089527),\n",
       "     'vf_loss': np.float64(6.716566329697768),\n",
       "     'vf_explained_var': np.float64(-0.01836751662194729),\n",
       "     'kl': np.float64(0.6361566481510332),\n",
       "     'entropy': np.float64(3.28274431626002),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)},\n",
       "   'pursuer_3': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(10.332651),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(6.854018593331178),\n",
       "     'policy_loss': np.float64(0.09294957247087345),\n",
       "     'vf_loss': np.float64(6.589247601230939),\n",
       "     'vf_explained_var': np.float64(0.09223279201736052),\n",
       "     'kl': np.float64(0.8591070479790991),\n",
       "     'entropy': np.float64(3.249066265951842),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)},\n",
       "   'pursuer_0': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(13.473987),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(8.077613924443721),\n",
       "     'policy_loss': np.float64(0.2053523147891004),\n",
       "     'vf_loss': np.float64(7.52750322073698),\n",
       "     'vf_explained_var': np.float64(0.006246730933586756),\n",
       "     'kl': np.float64(1.723792077577673),\n",
       "     'entropy': np.float64(3.565287616228064),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)},\n",
       "   'pursuer_5': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(6.1548185),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(9.17444859991471),\n",
       "     'policy_loss': np.float64(0.06458147976275844),\n",
       "     'vf_loss': np.float64(8.96580339173476),\n",
       "     'vf_explained_var': np.float64(0.117337599893411),\n",
       "     'kl': np.float64(0.7203187775623519),\n",
       "     'entropy': np.float64(3.3291859274109203),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)}},\n",
       "  'num_env_steps_sampled_for_evaluation_this_iter': 5000,\n",
       "  'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 4000,\n",
       "  'num_agent_steps_sampled': 24000,\n",
       "  'num_agent_steps_trained': 24000},\n",
       " 'env_runners': {'episode_reward_max': np.float64(-327.1718818863369),\n",
       "  'episode_reward_min': np.float64(-651.0568312071787),\n",
       "  'episode_reward_mean': np.float64(-521.166285331625),\n",
       "  'episode_len_mean': np.float64(500.0),\n",
       "  'episode_media': {},\n",
       "  'episodes_timesteps_total': 4000,\n",
       "  'policy_reward_min': {'pursuer_0': np.float64(-89.84811267121047),\n",
       "   'pursuer_1': np.float64(-112.50249861020443),\n",
       "   'pursuer_2': np.float64(-91.24802819055824),\n",
       "   'pursuer_3': np.float64(-107.97904688389782),\n",
       "   'pursuer_4': np.float64(-111.11837925890522),\n",
       "   'pursuer_5': np.float64(-212.1694086810163)},\n",
       "  'policy_reward_max': {'pursuer_0': np.float64(-7.318217051177981),\n",
       "   'pursuer_1': np.float64(-27.58393728642566),\n",
       "   'pursuer_2': np.float64(-48.906998868469366),\n",
       "   'pursuer_3': np.float64(-9.74873670808441),\n",
       "   'pursuer_4': np.float64(-8.12314018169635),\n",
       "   'pursuer_5': np.float64(-79.59940902683455)},\n",
       "  'policy_reward_mean': {'pursuer_0': np.float64(-49.37423269147564),\n",
       "   'pursuer_1': np.float64(-78.80312803094313),\n",
       "   'pursuer_2': np.float64(-79.74707460928532),\n",
       "   'pursuer_3': np.float64(-73.11886170322728),\n",
       "   'pursuer_4': np.float64(-69.70677086543017),\n",
       "   'pursuer_5': np.float64(-170.41621743126296)},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [np.float64(-327.1718818863369),\n",
       "    np.float64(-651.0568312071787),\n",
       "    np.float64(-620.2393821320601),\n",
       "    np.float64(-489.3921927436716),\n",
       "    np.float64(-410.1731976198102),\n",
       "    np.float64(-581.8466747940897),\n",
       "    np.float64(-575.4050666625217),\n",
       "    np.float64(-514.0450556073309)],\n",
       "   'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500],\n",
       "   'policy_pursuer_0_reward': [np.float64(-7.318217051177981),\n",
       "    np.float64(-89.84811267121047),\n",
       "    np.float64(-67.81082695742018),\n",
       "    np.float64(-71.91190472880133),\n",
       "    np.float64(-27.599276463261358),\n",
       "    np.float64(-31.960898365748204),\n",
       "    np.float64(-49.030113199761296),\n",
       "    np.float64(-49.51451209442428)],\n",
       "   'policy_pursuer_1_reward': [np.float64(-74.23225189379251),\n",
       "    np.float64(-68.39240528210188),\n",
       "    np.float64(-111.25103087508597),\n",
       "    np.float64(-111.30135546528297),\n",
       "    np.float64(-78.52141808791102),\n",
       "    np.float64(-46.64012674674057),\n",
       "    np.float64(-112.50249861020443),\n",
       "    np.float64(-27.58393728642566)],\n",
       "   'policy_pursuer_2_reward': [np.float64(-48.906998868469366),\n",
       "    np.float64(-91.24802819055824),\n",
       "    np.float64(-91.00219260111501),\n",
       "    np.float64(-88.84490928958596),\n",
       "    np.float64(-68.45956923680166),\n",
       "    np.float64(-89.8016870322555),\n",
       "    np.float64(-90.06113917726304),\n",
       "    np.float64(-69.65207247823379)],\n",
       "   'policy_pursuer_3_reward': [np.float64(-9.74873670808441),\n",
       "    np.float64(-107.97904688389782),\n",
       "    np.float64(-66.036674815765),\n",
       "    np.float64(-84.80410159987245),\n",
       "    np.float64(-68.74731589259862),\n",
       "    np.float64(-90.15617470942277),\n",
       "    np.float64(-88.53004439188066),\n",
       "    np.float64(-68.94879862429651)],\n",
       "   'policy_pursuer_4_reward': [np.float64(-8.12314018169635),\n",
       "    np.float64(-90.02686706189297),\n",
       "    np.float64(-111.09759313888274),\n",
       "    np.float64(-52.930512633294214),\n",
       "    np.float64(-9.520213959131988),\n",
       "    np.float64(-111.11837925890522),\n",
       "    np.float64(-68.83816491212305),\n",
       "    np.float64(-105.99929577751489)],\n",
       "   'policy_pursuer_5_reward': [np.float64(-178.84253718311618),\n",
       "    np.float64(-203.5623711175165),\n",
       "    np.float64(-173.04106374379046),\n",
       "    np.float64(-79.59940902683455),\n",
       "    np.float64(-157.3254039801049),\n",
       "    np.float64(-212.1694086810163),\n",
       "    np.float64(-166.44310637128856),\n",
       "    np.float64(-192.34643934643628)]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(1.3141188843139466),\n",
       "   'mean_inference_ms': np.float64(5.375802308425255),\n",
       "   'mean_action_processing_ms': np.float64(0.6453345740574232),\n",
       "   'mean_env_wait_ms': np.float64(6.361957551955223),\n",
       "   'mean_env_render_ms': np.float64(0.0)},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.005188087622324626),\n",
       "   'StateBufferConnector_ms': np.float64(0.002494951089223226),\n",
       "   'ViewRequirementAgentConnector_ms': np.float64(0.06810774405797322)},\n",
       "  'num_episodes': 8,\n",
       "  'episode_return_max': np.float64(-327.1718818863369),\n",
       "  'episode_return_min': np.float64(-651.0568312071787),\n",
       "  'episode_return_mean': np.float64(-521.166285331625),\n",
       "  'episodes_this_iter': 8},\n",
       " 'num_healthy_workers': 2,\n",
       " 'num_in_flight_async_sample_reqs': 0,\n",
       " 'num_remote_worker_restarts': 0,\n",
       " 'num_agent_steps_sampled': 24000,\n",
       " 'num_agent_steps_trained': 24000,\n",
       " 'num_env_steps_sampled': 4000,\n",
       " 'num_env_steps_trained': 4000,\n",
       " 'num_env_steps_sampled_this_iter': 4000,\n",
       " 'num_env_steps_trained_this_iter': 4000,\n",
       " 'num_env_steps_sampled_throughput_per_sec': 13.793587026868678,\n",
       " 'num_env_steps_trained_throughput_per_sec': 13.793587026868678,\n",
       " 'timesteps_total': 4000,\n",
       " 'num_env_steps_sampled_lifetime': 4000,\n",
       " 'num_agent_steps_sampled_lifetime': 24000,\n",
       " 'num_steps_trained_this_iter': 4000,\n",
       " 'agent_timesteps_total': 24000,\n",
       " 'timers': {'training_iteration_time_ms': 289989.883,\n",
       "  'restore_workers_time_ms': 0.089,\n",
       "  'training_step_time_ms': 289989.692,\n",
       "  'sample_time_ms': 28168.457,\n",
       "  'learn_time_ms': 261805.355,\n",
       "  'learn_throughput': 15.279,\n",
       "  'synch_weights_time_ms': 15.047,\n",
       "  'restore_eval_workers_time_ms': 0.015,\n",
       "  'evaluation_iteration_time_ms': 70558.346,\n",
       "  'evaluation_iteration_throughput': 70.863},\n",
       " 'counters': {'num_env_steps_sampled_for_evaluation_this_iter': 5000,\n",
       "  'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 4000,\n",
       "  'num_agent_steps_sampled': 24000,\n",
       "  'num_agent_steps_trained': 24000},\n",
       " 'done': False,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'date': '2024-09-03_18-00-53',\n",
       " 'timestamp': 1725386453,\n",
       " 'time_this_iter_s': 360.55868124961853,\n",
       " 'time_total_s': 360.55868124961853,\n",
       " 'pid': 1345171,\n",
       " 'hostname': 'e-bgbfbjbn-sfks6-0',\n",
       " 'node_ip': '10.42.142.216',\n",
       " 'config': {'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_for_main_process': 1,\n",
       "  'eager_tracing': True,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'torch_compile_learner': False,\n",
       "  'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
       "  'torch_compile_learner_dynamo_backend': 'inductor',\n",
       "  'torch_compile_learner_dynamo_mode': None,\n",
       "  'torch_compile_worker': False,\n",
       "  'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
       "  'torch_compile_worker_dynamo_mode': None,\n",
       "  'enable_rl_module_and_learner': False,\n",
       "  'enable_env_runner_and_connector_v2': False,\n",
       "  'env': '6_agent_env',\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  '_is_atari': None,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'action_mask_key': 'action_mask',\n",
       "  'env_runner_cls': None,\n",
       "  'num_env_runners': 2,\n",
       "  'num_envs_per_env_runner': 1,\n",
       "  'num_cpus_per_env_runner': 1,\n",
       "  'num_gpus_per_env_runner': 0,\n",
       "  'custom_resources_per_env_runner': {},\n",
       "  'validate_env_runners_after_construction': True,\n",
       "  'sample_timeout_s': 60.0,\n",
       "  '_env_to_module_connector': None,\n",
       "  'add_default_connectors_to_env_to_module_pipeline': True,\n",
       "  '_module_to_env_connector': None,\n",
       "  'add_default_connectors_to_module_to_env_pipeline': True,\n",
       "  'episode_lookback_horizon': 1,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'compress_observations': False,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'update_worker_filter_stats': True,\n",
       "  'use_worker_filter_stats': True,\n",
       "  'enable_connectors': True,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'num_learners': 0,\n",
       "  'num_gpus_per_learner': 0,\n",
       "  'num_cpus_per_learner': 1,\n",
       "  'local_gpu_idx': 0,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'grad_clip': None,\n",
       "  'grad_clip_by': 'global_norm',\n",
       "  'train_batch_size': 4000,\n",
       "  'train_batch_size_per_learner': None,\n",
       "  'model': {'_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_weights_initializer': None,\n",
       "   'fcnet_weights_initializer_config': None,\n",
       "   'fcnet_bias_initializer': None,\n",
       "   'fcnet_bias_initializer_config': None,\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'conv_kernel_initializer': None,\n",
       "   'conv_kernel_initializer_config': None,\n",
       "   'conv_bias_initializer': None,\n",
       "   'conv_bias_initializer_config': None,\n",
       "   'conv_transpose_kernel_initializer': None,\n",
       "   'conv_transpose_kernel_initializer_config': None,\n",
       "   'conv_transpose_bias_initializer': None,\n",
       "   'conv_transpose_bias_initializer_config': None,\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'post_fcnet_weights_initializer': None,\n",
       "   'post_fcnet_weights_initializer_config': None,\n",
       "   'post_fcnet_bias_initializer': None,\n",
       "   'post_fcnet_bias_initializer_config': None,\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   'lstm_weights_initializer': None,\n",
       "   'lstm_weights_initializer_config': None,\n",
       "   'lstm_bias_initializer': None,\n",
       "   'lstm_bias_initializer_config': None,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'encoder_latent_dim': None,\n",
       "   'always_check_shapes': False,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1},\n",
       "  '_learner_connector': None,\n",
       "  'add_default_connectors_to_learner_pipeline': True,\n",
       "  'learner_config_dict': {},\n",
       "  'optimizer': {},\n",
       "  'max_requests_in_flight_per_sampler_worker': 2,\n",
       "  '_learner_class': None,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'algorithm_config_overrides_per_module': {},\n",
       "  '_per_module_overrides': {},\n",
       "  'count_steps_by': 'env_steps',\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_mapping_fn': <function __main__.<lambda>(aid, *args, **kwargs)>,\n",
       "  'policies_to_train': None,\n",
       "  'policy_states_are_swappable': False,\n",
       "  'observation_fn': None,\n",
       "  'input_read_method': 'read_parquet',\n",
       "  'input_read_method_kwargs': {},\n",
       "  'input_read_schema': {},\n",
       "  'map_batches_kwargs': {},\n",
       "  'iter_batches_kwargs': {},\n",
       "  'prelearner_class': None,\n",
       "  'prelearner_module_synch_period': 10,\n",
       "  'dataset_num_iters_per_learner': None,\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': 1,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 120.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_force_reset_envs_before_iteration': True,\n",
       "  'evaluation_config': None,\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_env_runners': 0,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 0,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  '_run_training_always_in_thread': False,\n",
       "  '_evaluation_parallel_to_training_wo_thread': False,\n",
       "  'ignore_env_runner_failures': False,\n",
       "  'recreate_failed_env_runners': False,\n",
       "  'max_num_env_runner_restarts': 1000,\n",
       "  'delay_between_env_runner_restarts_s': 60.0,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_env_runner_failures_tolerance': 100,\n",
       "  'env_runner_health_probe_timeout_s': 30,\n",
       "  'env_runner_restore_timeout_s': 1800,\n",
       "  '_model_config_dict': {},\n",
       "  '_rl_module_spec': MultiAgentRLModuleSpec(marl_module_class=<class 'ray.rllib.core.rl_module.marl_module.MultiAgentRLModule'>, inference_only=False, module_specs={'pursuer_1': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None), 'pursuer_2': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None), 'pursuer_4': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None), 'pursuer_3': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None), 'pursuer_0': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None), 'pursuer_5': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None)}, load_state_path=None, modules_to_load=None),\n",
       "  '_AlgorithmConfig__prior_exploration_config': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_initialize_loss_from_dummy_batch': False,\n",
       "  '_dont_auto_sync_env_runner_states': False,\n",
       "  'simple_optimizer': True,\n",
       "  'policy_map_cache': -1,\n",
       "  'worker_cls': -1,\n",
       "  'synchronize_filters': -1,\n",
       "  'enable_async_evaluation': -1,\n",
       "  'custom_async_evaluation_function': -1,\n",
       "  '_enable_rl_module_api': -1,\n",
       "  'auto_wrap_old_gym_envs': -1,\n",
       "  'disable_env_checking': -1,\n",
       "  'always_attach_evaluation_results': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  '_disable_execution_plan_api': -1,\n",
       "  'lr_schedule': None,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'use_kl_loss': True,\n",
       "  'kl_coeff': 0.2,\n",
       "  'kl_target': 0.01,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'mini_batch_size_per_learner': None,\n",
       "  'num_sgd_iter': 30,\n",
       "  'shuffle_sequences': True,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'vf_share_layers': -1,\n",
       "  'lambda': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'policies': {'pursuer_1': (None, None, None, None),\n",
       "   'pursuer_2': (None, None, None, None),\n",
       "   'pursuer_4': (None, None, None, None),\n",
       "   'pursuer_3': (None, None, None, None),\n",
       "   'pursuer_0': (None, None, None, None),\n",
       "   'pursuer_5': (None, None, None, None)},\n",
       "  'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch'},\n",
       " 'time_since_restore': 360.55868124961853,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': np.float64(38.932853717026376),\n",
       "  'ram_util_percent': np.float64(9.687649880095922)}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resto_algo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Callbacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AlgorithmConfig.training() got an unexpected keyword argument 'trained_agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 55\u001b[0m\n\u001b[1;32m     34\u001b[0m register_env(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_agents\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_agent_env\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m _: ParallelPettingZooEnv(waterworld_v4\u001b[38;5;241m.\u001b[39mparallel_env(n_pursuers\u001b[38;5;241m=\u001b[39mnum_agents)))\n\u001b[1;32m     35\u001b[0m policies \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpursuer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents)}\n\u001b[1;32m     37\u001b[0m config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     38\u001b[0m     \u001b[43mget_trainable_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_agents\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_agent_env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Exact 1:1 mapping from AgentID to ModuleID.\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_mapping_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43maid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrl_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrl_module_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMultiAgentRLModuleSpec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSingleAgentRLModuleSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#.evaluation(\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#    evaluation_interval=1,\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#)\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMyCallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 55\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_agents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_agents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     58\u001b[0m algo \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mbuild()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ray/rllib/algorithms/ppo/ppo.py:275\u001b[0m, in \u001b[0;36mPPOConfig.training\u001b[0;34m(self, lr_schedule, use_critic, use_gae, lambda_, use_kl_loss, kl_coeff, kl_target, mini_batch_size_per_learner, sgd_minibatch_size, num_sgd_iter, shuffle_sequences, vf_loss_coeff, entropy_coeff, entropy_coeff_schedule, clip_param, vf_clip_param, grad_clip, vf_share_layers, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sets the training related configuration.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    This updated AlgorithmConfig object.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# Pass kwargs onto super's `training()` method.\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_critic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NotProvided:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_critic \u001b[38;5;241m=\u001b[39m use_critic\n",
      "\u001b[0;31mTypeError\u001b[0m: AlgorithmConfig.training() got an unexpected keyword argument 'trained_agents'"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation import Episode, RolloutWorker\n",
    "from ray.rllib.policy import Policy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(self, *, episode: Episode, **kwargs) -> None:\n",
    "        episode.hist_data[\"mean_agent_return\"] = []\n",
    "\n",
    "    def on_episode_end(self, *, episode: Episode, **kwargs) -> None:\n",
    "        episode.hist_data[\"mean_agent_return\"].append(\n",
    "            episode.total_reward / len(policies) )\n",
    "        #episode.hist_data['mod_return'] = episode.hist_data['episode_reward']/2\n",
    "    \n",
    "    def on_train_result(self, *, algorithm, result: dict, **kwargs) -> None:\n",
    "        #n_agents = len(policies)        \n",
    "        n_agents = len(result['info']['learner'])\n",
    "        result[\"num_agents\"] = n_agents\n",
    "        #result[\"mod_return\"] = result['info']['hist_stats'][\"episode_reward\"]/2\n",
    "        result[\"mod_return\"] = np.divide(\n",
    "            result['env_runners']['hist_stats']['episode_reward'], n_agents)\n",
    "\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "\n",
    "num_agents = 2\n",
    "register_env(f\"{num_agents}_agent_env\", lambda _: ParallelPettingZooEnv(waterworld_v4.parallel_env(n_pursuers=num_agents)))\n",
    "policies = {f\"pursuer_{i}\" for i in range(num_agents)}\n",
    "\n",
    "config = (\n",
    "    get_trainable_cls(\"PPO\")\n",
    "    .get_default_config()\n",
    "    .environment(f\"{num_agents}_agent_env\")\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        # Exact 1:1 mapping from AgentID to ModuleID.\n",
    "        policy_mapping_fn=(lambda aid, *args, **kwargs: aid),\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=MultiAgentRLModuleSpec(\n",
    "            module_specs={p: SingleAgentRLModuleSpec() for p in policies},\n",
    "        ),\n",
    "    )\n",
    "    #.evaluation(\n",
    "    #    evaluation_interval=1,\n",
    "    #)\n",
    "    .callbacks(MyCallbacks)\n",
    "    .training(trained_agents=num_agents)\n",
    ")\n",
    "\n",
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'custom_metrics': {},\n",
       " 'episode_media': {},\n",
       " 'info': {'learner': {'pursuer_0': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(2.6210372),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(8.687403419613839),\n",
       "     'policy_loss': np.float64(-0.005285420302243438),\n",
       "     'vf_loss': np.float64(8.691376147667567),\n",
       "     'vf_explained_var': np.float64(-7.799391945203145e-06),\n",
       "     'kl': np.float64(0.006563618332711485),\n",
       "     'entropy': np.float64(2.8232320780555407),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)},\n",
       "   'pursuer_1': {'learner_stats': {'allreduce_latency': np.float64(0.0),\n",
       "     'grad_gnorm': np.float32(1.7212684),\n",
       "     'cur_kl_coeff': np.float64(0.20000000000000004),\n",
       "     'cur_lr': np.float64(5.0000000000000016e-05),\n",
       "     'total_loss': np.float64(9.591291411717732),\n",
       "     'policy_loss': np.float64(-0.0064427012531571865),\n",
       "     'vf_loss': np.float64(9.59713891049226),\n",
       "     'vf_explained_var': np.float64(4.220716655254364e-05),\n",
       "     'kl': np.float64(0.002976043204379645),\n",
       "     'entropy': np.float64(2.884414534519116),\n",
       "     'entropy_coeff': np.float64(0.0)},\n",
       "    'model': {},\n",
       "    'custom_metrics': {},\n",
       "    'num_agent_steps_trained': np.float64(125.0),\n",
       "    'num_grad_updates_lifetime': np.float64(480.5),\n",
       "    'diff_num_grad_updates_vs_sampler_policy': np.float64(479.5)}},\n",
       "  'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 4000,\n",
       "  'num_agent_steps_sampled': 8000,\n",
       "  'num_agent_steps_trained': 8000},\n",
       " 'env_runners': {'episode_reward_max': np.float64(-165.02629530939475),\n",
       "  'episode_reward_min': np.float64(-289.7473406212149),\n",
       "  'episode_reward_mean': np.float64(-230.53974094624334),\n",
       "  'episode_len_mean': np.float64(500.0),\n",
       "  'episode_media': {},\n",
       "  'episodes_timesteps_total': 4000,\n",
       "  'policy_reward_min': {'pursuer_0': np.float64(-112.03651630179705),\n",
       "   'pursuer_1': np.float64(-221.37244872613195)},\n",
       "  'policy_reward_max': {'pursuer_0': np.float64(-28.910723104723512),\n",
       "   'pursuer_1': np.float64(-71.54037106148174)},\n",
       "  'policy_reward_mean': {'pursuer_0': np.float64(-65.22577256971653),\n",
       "   'pursuer_1': np.float64(-165.31396837652682)},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'mean_agent_return': [np.float64(-91.78844368163931),\n",
       "    np.float64(-144.87367031060745),\n",
       "    np.float64(-120.7425879385765),\n",
       "    np.float64(-112.4423399231702),\n",
       "    np.float64(-112.44106821259189),\n",
       "    np.float64(-143.5116369237169),\n",
       "    np.float64(-113.84606913997379),\n",
       "    np.float64(-82.51314765469738)],\n",
       "   'episode_reward': [np.float64(-183.57688736327862),\n",
       "    np.float64(-289.7473406212149),\n",
       "    np.float64(-241.485175877153),\n",
       "    np.float64(-224.8846798463404),\n",
       "    np.float64(-224.88213642518377),\n",
       "    np.float64(-287.0232738474338),\n",
       "    np.float64(-227.69213827994759),\n",
       "    np.float64(-165.02629530939475)],\n",
       "   'episode_lengths': [500, 500, 500, 500, 500, 500, 500, 500],\n",
       "   'policy_pursuer_0_reward': [np.float64(-112.03651630179705),\n",
       "    np.float64(-68.3748918950829),\n",
       "    np.float64(-68.4544707211679),\n",
       "    np.float64(-32.72226395986721),\n",
       "    np.float64(-70.79421805834343),\n",
       "    np.float64(-68.39199187660377),\n",
       "    np.float64(-72.12110464014638),\n",
       "    np.float64(-28.910723104723512)],\n",
       "   'policy_pursuer_1_reward': [np.float64(-71.54037106148174),\n",
       "    np.float64(-221.37244872613195),\n",
       "    np.float64(-173.0307051559852),\n",
       "    np.float64(-192.16241588647307),\n",
       "    np.float64(-154.0879183668402),\n",
       "    np.float64(-218.6312819708298),\n",
       "    np.float64(-155.57103363980116),\n",
       "    np.float64(-136.11557220467128)]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': np.float64(0.43803057272633217),\n",
       "   'mean_inference_ms': np.float64(1.529297371139412),\n",
       "   'mean_action_processing_ms': np.float64(0.2100298489289901),\n",
       "   'mean_env_wait_ms': np.float64(1.8404896768077144),\n",
       "   'mean_env_render_ms': np.float64(0.0)},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': np.float64(0.007826089859008789),\n",
       "   'StateBufferConnector_ms': np.float64(0.0026166439056396484),\n",
       "   'ViewRequirementAgentConnector_ms': np.float64(0.06763637065887451)},\n",
       "  'num_episodes': 8,\n",
       "  'episode_return_max': np.float64(-165.02629530939475),\n",
       "  'episode_return_min': np.float64(-289.7473406212149),\n",
       "  'episode_return_mean': np.float64(-230.53974094624334),\n",
       "  'episodes_this_iter': 8},\n",
       " 'num_healthy_workers': 2,\n",
       " 'num_in_flight_async_sample_reqs': 0,\n",
       " 'num_remote_worker_restarts': 0,\n",
       " 'num_agent_steps_sampled': 8000,\n",
       " 'num_agent_steps_trained': 8000,\n",
       " 'num_env_steps_sampled': 4000,\n",
       " 'num_env_steps_trained': 4000,\n",
       " 'num_env_steps_sampled_this_iter': 4000,\n",
       " 'num_env_steps_trained_this_iter': 4000,\n",
       " 'num_env_steps_sampled_throughput_per_sec': 44.59440704875901,\n",
       " 'num_env_steps_trained_throughput_per_sec': 44.59440704875901,\n",
       " 'timesteps_total': 4000,\n",
       " 'num_env_steps_sampled_lifetime': 4000,\n",
       " 'num_agent_steps_sampled_lifetime': 8000,\n",
       " 'num_steps_trained_this_iter': 4000,\n",
       " 'agent_timesteps_total': 8000,\n",
       " 'timers': {'training_iteration_time_ms': 89697.383,\n",
       "  'restore_workers_time_ms': 0.046,\n",
       "  'training_step_time_ms': 89697.251,\n",
       "  'sample_time_ms': 8158.678,\n",
       "  'learn_time_ms': 81514.168,\n",
       "  'learn_throughput': 49.071,\n",
       "  'synch_weights_time_ms': 23.998},\n",
       " 'counters': {'num_env_steps_sampled': 4000,\n",
       "  'num_env_steps_trained': 4000,\n",
       "  'num_agent_steps_sampled': 8000,\n",
       "  'num_agent_steps_trained': 8000},\n",
       " 'done': False,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'date': '2024-09-06_20-49-26',\n",
       " 'timestamp': 1725655766,\n",
       " 'time_this_iter_s': 89.70353984832764,\n",
       " 'time_total_s': 89.70353984832764,\n",
       " 'pid': 2066048,\n",
       " 'hostname': 'e-bgbfbjbn-sfks6-0',\n",
       " 'node_ip': '10.42.142.216',\n",
       " 'config': {'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_for_main_process': 1,\n",
       "  'eager_tracing': True,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'torch_compile_learner': False,\n",
       "  'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
       "  'torch_compile_learner_dynamo_backend': 'inductor',\n",
       "  'torch_compile_learner_dynamo_mode': None,\n",
       "  'torch_compile_worker': False,\n",
       "  'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
       "  'torch_compile_worker_dynamo_mode': None,\n",
       "  'enable_rl_module_and_learner': False,\n",
       "  'enable_env_runner_and_connector_v2': False,\n",
       "  'env': '2_agent_env',\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  '_is_atari': None,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'action_mask_key': 'action_mask',\n",
       "  'env_runner_cls': None,\n",
       "  'num_env_runners': 2,\n",
       "  'num_envs_per_env_runner': 1,\n",
       "  'num_cpus_per_env_runner': 1,\n",
       "  'num_gpus_per_env_runner': 0,\n",
       "  'custom_resources_per_env_runner': {},\n",
       "  'validate_env_runners_after_construction': True,\n",
       "  'sample_timeout_s': 60.0,\n",
       "  '_env_to_module_connector': None,\n",
       "  'add_default_connectors_to_env_to_module_pipeline': True,\n",
       "  '_module_to_env_connector': None,\n",
       "  'add_default_connectors_to_module_to_env_pipeline': True,\n",
       "  'episode_lookback_horizon': 1,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'compress_observations': False,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'update_worker_filter_stats': True,\n",
       "  'use_worker_filter_stats': True,\n",
       "  'enable_connectors': True,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'num_learners': 0,\n",
       "  'num_gpus_per_learner': 0,\n",
       "  'num_cpus_per_learner': 1,\n",
       "  'local_gpu_idx': 0,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'grad_clip': None,\n",
       "  'grad_clip_by': 'global_norm',\n",
       "  'train_batch_size': 4000,\n",
       "  'train_batch_size_per_learner': None,\n",
       "  'model': {'_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_weights_initializer': None,\n",
       "   'fcnet_weights_initializer_config': None,\n",
       "   'fcnet_bias_initializer': None,\n",
       "   'fcnet_bias_initializer_config': None,\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'conv_kernel_initializer': None,\n",
       "   'conv_kernel_initializer_config': None,\n",
       "   'conv_bias_initializer': None,\n",
       "   'conv_bias_initializer_config': None,\n",
       "   'conv_transpose_kernel_initializer': None,\n",
       "   'conv_transpose_kernel_initializer_config': None,\n",
       "   'conv_transpose_bias_initializer': None,\n",
       "   'conv_transpose_bias_initializer_config': None,\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'post_fcnet_weights_initializer': None,\n",
       "   'post_fcnet_weights_initializer_config': None,\n",
       "   'post_fcnet_bias_initializer': None,\n",
       "   'post_fcnet_bias_initializer_config': None,\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   'lstm_weights_initializer': None,\n",
       "   'lstm_weights_initializer_config': None,\n",
       "   'lstm_bias_initializer': None,\n",
       "   'lstm_bias_initializer_config': None,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'encoder_latent_dim': None,\n",
       "   'always_check_shapes': False,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1},\n",
       "  '_learner_connector': None,\n",
       "  'add_default_connectors_to_learner_pipeline': True,\n",
       "  'learner_config_dict': {},\n",
       "  'optimizer': {},\n",
       "  'max_requests_in_flight_per_sampler_worker': 2,\n",
       "  '_learner_class': None,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'algorithm_config_overrides_per_module': {},\n",
       "  '_per_module_overrides': {},\n",
       "  'count_steps_by': 'env_steps',\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_mapping_fn': <function __main__.<lambda>(aid, *args, **kwargs)>,\n",
       "  'policies_to_train': None,\n",
       "  'policy_states_are_swappable': False,\n",
       "  'observation_fn': None,\n",
       "  'input_read_method': 'read_parquet',\n",
       "  'input_read_method_kwargs': {},\n",
       "  'input_read_schema': {},\n",
       "  'map_batches_kwargs': {},\n",
       "  'iter_batches_kwargs': {},\n",
       "  'prelearner_class': None,\n",
       "  'prelearner_module_synch_period': 10,\n",
       "  'dataset_num_iters_per_learner': None,\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 120.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_force_reset_envs_before_iteration': True,\n",
       "  'evaluation_config': None,\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_env_runners': 0,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 0,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  '_run_training_always_in_thread': False,\n",
       "  '_evaluation_parallel_to_training_wo_thread': False,\n",
       "  'ignore_env_runner_failures': False,\n",
       "  'recreate_failed_env_runners': False,\n",
       "  'max_num_env_runner_restarts': 1000,\n",
       "  'delay_between_env_runner_restarts_s': 60.0,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_env_runner_failures_tolerance': 100,\n",
       "  'env_runner_health_probe_timeout_s': 30,\n",
       "  'env_runner_restore_timeout_s': 1800,\n",
       "  '_model_config_dict': {},\n",
       "  '_rl_module_spec': MultiAgentRLModuleSpec(marl_module_class=<class 'ray.rllib.core.rl_module.marl_module.MultiAgentRLModule'>, inference_only=False, module_specs={'pursuer_0': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None), 'pursuer_1': SingleAgentRLModuleSpec(module_class=None, observation_space=None, action_space=None, inference_only=False, model_config_dict=None, catalog_class=None, load_state_path=None)}, load_state_path=None, modules_to_load=None),\n",
       "  '_AlgorithmConfig__prior_exploration_config': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_initialize_loss_from_dummy_batch': False,\n",
       "  '_dont_auto_sync_env_runner_states': False,\n",
       "  'simple_optimizer': True,\n",
       "  'policy_map_cache': -1,\n",
       "  'worker_cls': -1,\n",
       "  'synchronize_filters': -1,\n",
       "  'enable_async_evaluation': -1,\n",
       "  'custom_async_evaluation_function': -1,\n",
       "  '_enable_rl_module_api': -1,\n",
       "  'auto_wrap_old_gym_envs': -1,\n",
       "  'disable_env_checking': -1,\n",
       "  'always_attach_evaluation_results': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  '_disable_execution_plan_api': -1,\n",
       "  'lr_schedule': None,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'use_kl_loss': True,\n",
       "  'kl_coeff': 0.2,\n",
       "  'kl_target': 0.01,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'mini_batch_size_per_learner': None,\n",
       "  'num_sgd_iter': 30,\n",
       "  'shuffle_sequences': True,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'vf_share_layers': -1,\n",
       "  'lambda': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'policies': {'pursuer_0': (None, None, None, None),\n",
       "   'pursuer_1': (None, None, None, None)},\n",
       "  'callbacks': __main__.MyCallbacks,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch'},\n",
       " 'time_since_restore': 89.70353984832764,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': np.float64(49.089999999999996),\n",
       "  'ram_util_percent': np.float64(9.74846153846154)},\n",
       " 'num_agents': 2,\n",
       " 'mod_return': array([-112.28477977, -105.30352136, -135.2337303 , -150.27593418,\n",
       "        -134.60172764, -112.50189289, -110.97125765, -112.29131475,\n",
       "        -112.50852581, -144.84313125, -122.08270215, -133.03073351,\n",
       "         -88.58860856,  -92.54429235, -121.68296486, -112.93834275])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(-224.56955954147745),\n",
       " np.float64(-210.60704272678404),\n",
       " np.float64(-270.4674606087424),\n",
       " np.float64(-300.55186835564245),\n",
       " np.float64(-269.2034552824137),\n",
       " np.float64(-225.00378578885667),\n",
       " np.float64(-221.94251529451898),\n",
       " np.float64(-224.5826295071694),\n",
       " np.float64(-225.01705161938244),\n",
       " np.float64(-289.68626249930134),\n",
       " np.float64(-244.16540430415856),\n",
       " np.float64(-266.06146701715795),\n",
       " np.float64(-177.1772171119893),\n",
       " np.float64(-185.088584697531),\n",
       " np.float64(-243.36592972517653),\n",
       " np.float64(-225.87668550926975)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['env_runners']['hist_stats']['episode_reward']\n",
    "np.divide(results['env_runners']['hist_stats']['episode_reward'],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results['info']['learner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
